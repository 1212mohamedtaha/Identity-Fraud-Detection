{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Or0hIyYY8Dvo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usfhoULw954g",
    "outputId": "7d2a45ae-c20e-44d0-9696-eb710d1ff59d"
   },
   "outputs": [],
   "source": [
    "# cd /content/drive/MyDrive/Second_English_Data/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eRpVZ1bW-JPn"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SB2Q5B9-WV1"
   },
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VXabnZGR-Q38"
   },
   "outputs": [],
   "source": [
    "# coding = utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "#from src.NNModule.utils import Attn, batch_embedding_lookup\n",
    "#from src.config import FRAUD, NON_FRAUD, ManagerRewardDiscount, WorkerRewardDiscount, EPS, Pad_Query_Node\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, score_method, agent_state_size, answer_node_emb_size):\n",
    "        super(Agent, self).__init__()\n",
    "        self.policy_network = Attn(score_method, answer_node_emb_size, agent_state_size)\n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(agent_state_size, agent_state_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(agent_state_size // 2, 1)\n",
    "        )\n",
    "        # two vector to represent the fraud and non-fraud actions\n",
    "        self.fraud_embed = Parameter(torch.Tensor(answer_node_emb_size))\n",
    "        self.fraud_embed.data.uniform_(-1, 1)\n",
    "        self.non_fraud_embed = Parameter(torch.Tensor(answer_node_emb_size))\n",
    "        self.non_fraud_embed.data.uniform_(-1, 1)\n",
    "\n",
    "    def forward(self, agent_state, answer_nodes, graph_node_embedding):\n",
    "        \"\"\"\n",
    "        :param agent_state: (batch_size, agent_state_size)\n",
    "        :param answer_nodes: (batch_size, answer_node_num)\n",
    "        :param graph_node_embedding: (batch_size, node_num, node_feature_size)\n",
    "        :return:\n",
    "        values: (batch_size,)\n",
    "        logits: (batch, answer_node_num + 2)\n",
    "        \"\"\"\n",
    "        values = self.value_network(agent_state).squeeze(-1)\n",
    "\n",
    "        batch_size = agent_state.shape[0]\n",
    "        answer_node_embedding = batch_embedding_lookup(graph_node_embedding, answer_nodes)\n",
    "        actions_embedding = torch.cat((answer_node_embedding,\n",
    "                                       self.fraud_embed.repeat(batch_size, 1, 1),\n",
    "                                       self.non_fraud_embed.repeat(batch_size, 1, 1)),\n",
    "                                      dim=1)\n",
    "        logits = self.policy_network(actions_embedding, agent_state)\n",
    "\n",
    "        return values, logits\n",
    "\n",
    "\n",
    "class Manager(nn.Module):\n",
    "    def __init__(self, score_method, manager_state_size, worker_state_size):\n",
    "        super(Manager, self).__init__()\n",
    "        self.policy_network = Attn(score_method, worker_state_size, manager_state_size)\n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(manager_state_size, manager_state_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(manager_state_size // 2, 1)\n",
    "        )\n",
    "\n",
    "        # two vector to represent the fraud and non-fraud actions\n",
    "        self.fraud_embed = Parameter(torch.Tensor(worker_state_size))\n",
    "        self.fraud_embed.data.uniform_(-1, 1)\n",
    "        self.non_fraud_embed = Parameter(torch.Tensor(worker_state_size))\n",
    "        self.non_fraud_embed.data.uniform_(-1, 1)\n",
    "\n",
    "    def forward(self, manager_state, workers_state):\n",
    "        \"\"\"\n",
    "        :param manager_state: (batch_size, manager_state_size)\n",
    "        :param workers_state: (batch_size, personal_node_num, worker_sate_size)\n",
    "        :return:\n",
    "        values: (batch_size,)\n",
    "        logits: (batch_size, personal_node_num + 2)\n",
    "        \"\"\"\n",
    "        values = self.value_network(manager_state).squeeze(-1)\n",
    "\n",
    "        batch_size = manager_state.shape[0]\n",
    "        actions_embedding = torch.cat((workers_state,\n",
    "                                       self.fraud_embed.repeat(batch_size, 1, 1),\n",
    "                                       self.non_fraud_embed.repeat(batch_size, 1, 1)),\n",
    "                                      dim=1)\n",
    "        logits = self.policy_network(actions_embedding, manager_state)\n",
    "\n",
    "        return values, logits\n",
    "\n",
    "\n",
    "class Workers(nn.Module):\n",
    "    def __init__(self, score_method, worker_state_size, answer_node_emb_size):\n",
    "        super(Workers, self).__init__()\n",
    "        self.policy_networks = Attn(score_method, answer_node_emb_size, worker_state_size)\n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(worker_state_size, worker_state_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(worker_state_size // 2, 1)\n",
    "        )\n",
    "\n",
    "        # two vector to represent the fraud and non-fraud actions\n",
    "        self.fraud_embed = Parameter(torch.Tensor(answer_node_emb_size))\n",
    "        self.fraud_embed.data.uniform_(-1, 1)\n",
    "        self.non_fraud_embed = Parameter(torch.Tensor(answer_node_emb_size))\n",
    "        self.non_fraud_embed.data.uniform_(-1, 1)\n",
    "\n",
    "    def forward(self, workers_state, answer_nodes, graph_node_embedding):\n",
    "        \"\"\"\n",
    "        :param workers_state: (batch_size, personal_node_num, worker_state_size)\n",
    "        :param answer_nodes: (batch_size, personal_node_num, answer_node_num)\n",
    "        :param graph_node_embedding: (batch_size, node_num, node_feature_size)\n",
    "        :return:\n",
    "        values: (batch_size, personal_node_num)\n",
    "        logits: (batch_size, personal_node_num, answer_node_num + 2)\n",
    "        \"\"\"\n",
    "        values = self.value_network(workers_state).squeeze(-1)\n",
    "\n",
    "        batch_size = answer_nodes.shape[0]\n",
    "        personal_node_num = answer_nodes.shape[1]\n",
    "        answer_node_num = answer_nodes.shape[2]\n",
    "\n",
    "        answer_nodes = answer_nodes.reshape(batch_size, -1)\n",
    "        answer_node_embedding = batch_embedding_lookup(graph_node_embedding, answer_nodes)\n",
    "        answer_node_embedding = answer_node_embedding.reshape(batch_size, personal_node_num, answer_node_num, -1)\n",
    "        actions_embedding = torch.cat((answer_node_embedding,\n",
    "                                       self.fraud_embed.repeat(batch_size, personal_node_num, 1, 1),\n",
    "                                       self.non_fraud_embed.repeat(batch_size, personal_node_num, 1, 1)),\n",
    "                                      dim=2)\n",
    "\n",
    "        workers_state = workers_state.reshape(batch_size * personal_node_num, -1)\n",
    "        actions_embedding = actions_embedding.reshape(batch_size * personal_node_num, answer_node_num + 2, -1)\n",
    "\n",
    "        logits = self.policy_networks(actions_embedding, workers_state)\n",
    "        logits = logits.reshape(batch_size, personal_node_num, -1)\n",
    "        return values, logits\n",
    "\n",
    "\n",
    "def sample_from_prob_matrix(prob_matrix, sample_flag):\n",
    "    \"\"\"\n",
    "    Sample n times based on prob matrix. The prob in i-th experiments is the i-th row of prob matrix.\n",
    "    :param prob_matrix: (n_times, m_items)\n",
    "    :param sample_flag: str, max, random, sample\n",
    "    :return: choices: (n_times,)\n",
    "    \"\"\"\n",
    "    if sample_flag == \"sample\":\n",
    "        cumulative_prob = prob_matrix.cumsum(axis=1)\n",
    "        uniform = np.random.rand(len(cumulative_prob), 1)\n",
    "        choices = (uniform < cumulative_prob).argmax(axis=1)\n",
    "    elif sample_flag == \"max\":\n",
    "        choices = prob_matrix.argmax(axis=1)\n",
    "    elif sample_flag == \"random\":\n",
    "        new_prob_matrix = np.asarray(prob_matrix > 1000 * EPS, dtype=np.float32)\n",
    "        items_matrix = new_prob_matrix.sum(axis=1, keepdims=True) + EPS\n",
    "        choices = sample_from_prob_matrix(new_prob_matrix / items_matrix, sample_flag=\"sample\")\n",
    "    else:\n",
    "        raise ValueError('Unknown Sample Flag.')\n",
    "    return choices\n",
    "\n",
    "\n",
    "def sample_hierarchy_rl(manager_action_probs, workers_action_probs, manager_actions, workers_actions, sample_flag):\n",
    "    \"\"\"\n",
    "    Assume all inputs are np.array\n",
    "    :param manager_action_probs: (batch_size, personal_node_num + 2)\n",
    "    :param workers_action_probs: (batch_size, personal_node_num, answer_node_num + 2)\n",
    "    :param manager_actions: (batch_size, personal_node_num)\n",
    "    :param workers_actions: (batch_size, personal_node_num, answer_node_num)\n",
    "    :param sample_flag:\n",
    "    :return:\n",
    "    manager_sample_idx: (batch_size,)\n",
    "    manager_sample_result: (batch_size,)\n",
    "    workers_sample_idx: (batch_size, personal_node_num)\n",
    "    workers_sample_result: (batch_size, personal_node_num)\n",
    "    \"\"\"\n",
    "    manager_sample_idx = sample_from_prob_matrix(manager_action_probs, sample_flag)\n",
    "\n",
    "    batch_size = workers_action_probs.shape[0]\n",
    "    personal_node_num = workers_action_probs.shape[1]\n",
    "    workers_action_probs = workers_action_probs.reshape(batch_size * personal_node_num, -1)\n",
    "    workers_sample_idx = sample_from_prob_matrix(workers_action_probs, sample_flag)\n",
    "\n",
    "    manager_terminal_actions = np.tile(np.asarray([FRAUD, NON_FRAUD]), (batch_size, 1))\n",
    "    manager_actions = np.concatenate((manager_actions, manager_terminal_actions), axis=1)\n",
    "    manager_sample_result = manager_actions[np.arange(manager_actions.shape[0]), manager_sample_idx]\n",
    "\n",
    "    worker_terminal_actions = np.tile(np.asarray([FRAUD, NON_FRAUD]), (batch_size, personal_node_num, 1))\n",
    "    workers_actions = np.concatenate((workers_actions, worker_terminal_actions), axis=2)\n",
    "    workers_actions = workers_actions.reshape(batch_size * personal_node_num, -1)\n",
    "    workers_sample_result = workers_actions[np.arange(workers_actions.shape[0]), workers_sample_idx]\n",
    "\n",
    "    workers_sample_result = workers_sample_result.reshape(batch_size, personal_node_num)\n",
    "    workers_sample_idx = workers_sample_idx.reshape(batch_size, personal_node_num)\n",
    "\n",
    "    return manager_sample_idx, manager_sample_result, workers_sample_idx, workers_sample_result\n",
    "\n",
    "\n",
    "def sample_flatten_rl(action_probs, answer_nodes, query_nodes, sample_flag):\n",
    "    \"\"\"\n",
    "    Assume all inputs are np.array\n",
    "    :param action_probs: (batch_size, answer_node_num + 2)\n",
    "    :param answer_nodes: (batch_size, answer_node_num)\n",
    "    :param query_nodes: (batch_size, answer_node_num)\n",
    "    :param sample_flag:\n",
    "    :return:\n",
    "    sample_idx: (batch_size,)\n",
    "    sample_content: (batch_size, 2)\n",
    "    \"\"\"\n",
    "    sample_idx = sample_from_prob_matrix(action_probs, sample_flag)\n",
    "    batch_size = action_probs.shape[0]\n",
    "\n",
    "    terminal_actions = np.tile(np.asarray([FRAUD, NON_FRAUD]), (batch_size, 1))\n",
    "    pad_query_nodes = np.tile(np.asarray([Pad_Query_Node, Pad_Query_Node]), (batch_size, 1))\n",
    "    answer_nodes = np.concatenate((answer_nodes, terminal_actions), axis=1)\n",
    "    query_nodes = np.concatenate((query_nodes, pad_query_nodes), axis=1)\n",
    "    sample_query = query_nodes[np.arange(query_nodes.shape[0]), sample_idx]\n",
    "    sample_result = answer_nodes[np.arange(answer_nodes.shape[0]), sample_idx]\n",
    "\n",
    "    sample_content = np.concatenate((sample_query[:, np.newaxis], sample_result[:, np.newaxis]), axis=1)\n",
    "\n",
    "    return sample_idx, sample_content\n",
    "\n",
    "\n",
    "def get_hierarchy_rl_returns(rewards, masks):\n",
    "    \"\"\"\n",
    "    :param rewards: (batch_size, 1 + workers_num, time_steps)\n",
    "    :param masks: (batch_size, 1 + workers_num, time_steps)\n",
    "    :return: returns: (batch_size, 1 + workers_num, time_steps)\n",
    "    \"\"\"\n",
    "    masks = masks.to(torch.uint8)\n",
    "\n",
    "    gamma = torch.zeros_like(rewards)\n",
    "    gamma[:, 0, :] = ManagerRewardDiscount\n",
    "    gamma[:, 1:, :] = WorkerRewardDiscount\n",
    "    gamma[1 - masks] = torch.ones_like(rewards)[1 - masks]\n",
    "\n",
    "    batch_size = rewards.shape[0]\n",
    "    agent_num = rewards.shape[1]\n",
    "    time_steps = rewards.shape[2]\n",
    "    rewards = rewards.reshape(batch_size * agent_num, -1)\n",
    "    masks = masks.reshape(batch_size * agent_num, -1)\n",
    "    gamma = gamma.reshape(batch_size * agent_num, -1)\n",
    "\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    running_returns = torch.zeros_like(rewards[:, 0])\n",
    "    for t in reversed(range(0, time_steps)):\n",
    "        running_returns = rewards[:, t] * masks[:, t].to(running_returns.dtype) + gamma[:, t] * running_returns\n",
    "        returns[:, t] = running_returns\n",
    "\n",
    "    returns = returns.reshape(batch_size, agent_num, time_steps)\n",
    "    masks = masks.reshape(batch_size, agent_num, time_steps)\n",
    "\n",
    "    # Mask the invalid items\n",
    "    returns = returns * masks.to(returns.dtype)\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "def get_flatten_rl_returns(rewards, masks):\n",
    "    \"\"\"\n",
    "    :param rewards: (batch_size, time_steps)\n",
    "    :param masks: (batch_size, time_steps)\n",
    "    :return: returns: (batch_size, time_steps)\n",
    "    \"\"\"\n",
    "    gamma = ManagerRewardDiscount\n",
    "    time_steps = rewards.shape[1]\n",
    "\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    running_returns = torch.zeros_like(rewards[:, 0])\n",
    "    for t in reversed(range(0, time_steps)):\n",
    "        running_returns = rewards[:, t] * masks[:, t].to(running_returns.dtype) + gamma * running_returns\n",
    "        returns[:, t] = running_returns\n",
    "\n",
    "    # Mask the invalid items\n",
    "    returns = returns * masks.to(returns.dtype)\n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WgG904He-MJk"
   },
   "outputs": [],
   "source": [
    "# coding = utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from src.config import EPS\n",
    "\n",
    "\n",
    "def batch_embedding_lookup(embeddings, indices):\n",
    "    \"\"\"\n",
    "    Look up from a batch of embedding matrices.\n",
    "    :param embeddings: (batch_size, num_words, embedding_size)\n",
    "    :param indices: (batch_size, num_inds)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    shape = embeddings.shape\n",
    "    batch_size = shape[0]\n",
    "    num_words = shape[1]\n",
    "    embed_size = shape[2]\n",
    "\n",
    "    offset = torch.reshape(torch. arange(batch_size) * num_words, (batch_size, 1)).to(dtype=indices.dtype,\n",
    "                                                                                     device=indices.device)\n",
    "    flat_embeddings = torch.reshape(embeddings, (-1, embed_size)) # first as it is.\n",
    "    flat_indices = torch.reshape(indices + offset, (-1,))\n",
    "    embeds = torch.reshape(F.embedding(flat_indices, flat_embeddings), (batch_size, -1, embed_size)) # from pytorch\n",
    "    return embeds\n",
    "\n",
    "\n",
    "def mask_softmax(input_tensor, mask, dim):\n",
    "    \"\"\"\n",
    "    Softmax with mask to input tensor.\n",
    "    :param input_tensor: Tensor with any shape\n",
    "    :param mask: same shape with input tensor\n",
    "    :param dim: a dimension along which softmax will be computed\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    exps = torch.exp(input_tensor)\n",
    "    masked_exps = exps * mask.to(exps.dtype)\n",
    "    masked_sums = masked_exps.sum(dim, keepdim=True) + EPS\n",
    "    return masked_exps / masked_sums\n",
    "\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, encode_hidden_size, decode_hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        if method.lower() not in [\"dotted\", \"general\", \"concat\"]:\n",
    "            raise RuntimeError(\"Attention methods should be dotted, general or concat but get {}!\".format(method))\n",
    "        if method.lower() == \"dotted\" and encode_hidden_size != decode_hidden_size:\n",
    "            raise RuntimeError(\"In dotted attention, the encode_hidden_size should equal to decode_hidden_size.\")\n",
    "\n",
    "        self.method = method.lower()\n",
    "        self.encode_hidden_size = encode_hidden_size\n",
    "        self.decode_hidden_size = decode_hidden_size\n",
    "\n",
    "        if self.method == \"general\":\n",
    "            self.attn = nn.Linear(self.encode_hidden_size, self.decode_hidden_size)\n",
    "        elif self.method == \"concat\":\n",
    "            self.attn = nn.Sequential(\n",
    "                nn.Linear((self.encode_hidden_size + self.decode_hidden_size),\n",
    "                          (self.encode_hidden_size + self.decode_hidden_size) // 2),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear((self.encode_hidden_size + self.decode_hidden_size) // 2, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, encode_outputs, decode_state):\n",
    "        \"\"\"\n",
    "        :param encode_outputs: (batch, output_length, encode_hidden_size)\n",
    "        :param decode_state: (batch, decode_hidden_size)\n",
    "        :return: energy: (batch, output_length)\n",
    "        \"\"\"\n",
    "        output_length = encode_outputs.size(1)\n",
    "        if self.method == \"concat\":\n",
    "            decode_state_temp = decode_state.unsqueeze(1)\n",
    "            decode_state_temp = decode_state_temp.expand(-1, output_length, -1)\n",
    "            cat_encode_decode = torch.cat([encode_outputs, decode_state_temp], 2)\n",
    "            energy = self.attn(cat_encode_decode).squeeze(-1)\n",
    "        elif self.method == \"general\":\n",
    "            decode_state_temp = decode_state.unsqueeze(1)\n",
    "            mapped_encode_outputs = self.attn(encode_outputs)\n",
    "            energy = torch.sum(decode_state_temp * mapped_encode_outputs, 2)\n",
    "        else:\n",
    "            decode_state_temp = decode_state.unsqueeze(1)\n",
    "            energy = torch.sum(decode_state_temp * encode_outputs, 2)\n",
    "        return energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "INU5ChEu-gW3"
   },
   "outputs": [],
   "source": [
    "# coding = utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from src.NNModule.utils import batch_embedding_lookup\n",
    "#from src.config import EPS\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    \"\"\" A pytorch implementation of Message Passing Network \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 node_emb_size_list,\n",
    "                 msg_agg):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.mp_iters = len(node_emb_size_list) - 1\n",
    "        self.linear_cells = nn.ModuleList([nn.Linear(in_features, out_features) for in_features, out_features in\n",
    "                                           zip(node_emb_size_list[:-1], node_emb_size_list[1:])])\n",
    "        self.msg_agg = msg_agg\n",
    "\n",
    "    def embed_edge(self, node_embedding, edges, iter_idx):\n",
    "        \"\"\"\n",
    "        Compute embedding of a edge (sender_node, relation_label).\n",
    "        :param node_embedding: (batch_size, max_node_num, feature_size)\n",
    "        :param edges: each edge is a tuple of (sender_node, relation_label, receiver_node).\n",
    "        (batch_size, max_edge_num, 3)\n",
    "        :param iter_idx: the iter_idx-th iteration of graph\n",
    "        :return: edge_embeds: (batch_size, max_edge_num, feature_size)\n",
    "        \"\"\"\n",
    "        sender_embeds = batch_embedding_lookup(node_embedding, edges[:, :, 0])\n",
    "        edge_embeds = torch.tanh(self.linear_cells[iter_idx](sender_embeds))\n",
    "        return edge_embeds\n",
    "\n",
    "    def pass_message(self, node_edges, node_edge_mask, edge_embeds):\n",
    "        \"\"\"\n",
    "        Compute new node embedding by summing edge embeds (message) of neighboring nodes.\n",
    "        :param node_edges: ids of neighboring edges of each node where id is row index in edge_embeds\n",
    "        (batch_size, max_node_num, max_node_edge_num)\n",
    "        :param node_edge_mask: mask for node_edges. (batch_size, max_node_num, max_node_edge_num)\n",
    "        :param edge_embeds: (batch_size, max_edge_num, feature_size)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        num_neighbors = torch.sum(node_edge_mask.to(torch.float32), 2, keepdim=True) + EPS\n",
    "\n",
    "        shape = node_edges.shape\n",
    "        batch_size = shape[0]\n",
    "        node_num = shape[1]\n",
    "        edge_embed_size = edge_embeds.shape[-1]\n",
    "\n",
    "        # Gather neighboring edge embeddings\n",
    "        neighbors = torch.reshape(node_edges, (batch_size, -1))  # (batch_size, max_node_num * max_node_edge_num)\n",
    "        embeds = batch_embedding_lookup(edge_embeds,\n",
    "                                        neighbors)  # (batch_size, max_node_num * max_node_edge_num, feature_size)\n",
    "        embeds = torch.reshape(embeds, (batch_size, node_num, -1, edge_embed_size))\n",
    "        mask = torch.unsqueeze(node_edge_mask, 3)  # (batch_size, max_node_num, max_node_edge_num, 1)\n",
    "        embeds = embeds * mask.to(embeds.dtype)\n",
    "\n",
    "        # (batch_size, max_node_num, feature_size)\n",
    "        if self.msg_agg == 'sum':\n",
    "            new_node_embed = torch.sum(embeds, 2)\n",
    "        elif self.msg_agg == 'avg':\n",
    "            new_node_embed = torch.sum(embeds, 2) / num_neighbors\n",
    "        elif self.msg_agg == 'max':\n",
    "            new_node_embed, _ = torch.max(embeds, 2)\n",
    "        else:\n",
    "            raise ValueError('Unknown message aggregation method')\n",
    "\n",
    "        return new_node_embed\n",
    "\n",
    "    def mp(self, curr_node_embedding, edges, iter_idx, node_edges, node_edge_mask):\n",
    "        edge_embeds = self.embed_edge(curr_node_embedding, edges, iter_idx)\n",
    "        new_node_embed = self.pass_message(node_edges, node_edge_mask, edge_embeds)\n",
    "        return new_node_embed\n",
    "\n",
    "    def forward(self, initial_node_embed, edges, node_edges, node_edge_mask):\n",
    "        \"\"\"\n",
    "        :param initial_node_embed: (batch_size, max_node_num, feature_size)\n",
    "        :param edges: (batch_size, max_edge_num, 3)\n",
    "        :param node_edges: (batch_size, max_node_num, max_node_edge_num)\n",
    "        :param node_edge_mask: (batch_size, max_node_num, max_node_edge_num)\n",
    "        :return: final_node_embed: (batch_size, max_node_num, feature_size)\n",
    "        \"\"\"\n",
    "        node_embed_list = [initial_node_embed]\n",
    "        for iter_idx in range(self.mp_iters):\n",
    "            node_embed_list.append(self.mp(node_embed_list[-1], edges, iter_idx, node_edges, node_edge_mask))\n",
    "        final_node_embed = torch.cat(node_embed_list, 2)\n",
    "        return final_node_embed\n",
    "\n",
    "\n",
    "class WorkersStateTracker(nn.Module):\n",
    "    \"\"\" Concat personal nodes embedding and hand-crafted features to get the workers dialogue state \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WorkersStateTracker, self).__init__()\n",
    "\n",
    "    def forward(self, known_one_hot, unknown_one_hot, known_differ_one_hot, workers_qa_turn_one_hot,\n",
    "                workers_max_qa_turn_one_hot, personal_nodes, final_node_embed):\n",
    "        \"\"\"\n",
    "        :param known_one_hot: (batch_size, personal_node_num, feature_size)\n",
    "        :param unknown_one_hot: (batch_size, personal_node_num, feature_size)\n",
    "        :param known_differ_one_hot: (batch_size, personal_node_num, feature_size)\n",
    "        :param workers_qa_turn_one_hot: (batch_size, personal_node_num, feature_size)\n",
    "        :param workers_max_qa_turn_one_hot: (batch_size, personal_node_num, feature_size)\n",
    "        :param personal_nodes: (batch_size, personal_node_num)\n",
    "        :param final_node_embed: (batch_size, feature_size)\n",
    "        :return: workers_state: (batch_size, personal_node_num, feature_size)\n",
    "        \"\"\"\n",
    "        personal_node_embed = batch_embedding_lookup(final_node_embed, personal_nodes)\n",
    "\n",
    "        workers_state = torch.cat((known_one_hot,\n",
    "                                   unknown_one_hot,\n",
    "                                   known_differ_one_hot,\n",
    "                                   workers_qa_turn_one_hot,\n",
    "                                   workers_max_qa_turn_one_hot,\n",
    "                                   personal_node_embed),\n",
    "                                  2)\n",
    "        return workers_state\n",
    "\n",
    "\n",
    "class ManagerStateTracker(nn.Module):\n",
    "    \"\"\" Aggregate personal nodes embedding and hand-crafted features to get the manager dialogue state \"\"\"\n",
    "\n",
    "    def __init__(self, personal_node_emb_size, manager_agg_size, msg_agg):\n",
    "        super(ManagerStateTracker, self).__init__()\n",
    "\n",
    "        self.msg_agg = msg_agg\n",
    "        self.transfer = nn.Sequential(nn.Linear(personal_node_emb_size, manager_agg_size),\n",
    "                                      nn.Tanh())\n",
    "\n",
    "    def forward(self, feasible_personal_info_nodes, workers_decision,\n",
    "                known_one_hot, unknown_one_hot, known_differ_one_hot,\n",
    "                total_qa_turn_one_hot, personal_nodes, final_node_embed):\n",
    "        \"\"\"\n",
    "        :param feasible_personal_info_nodes: (batch_size, personal_node_num)\n",
    "        :param workers_decision: (batch_size, personal_node_num, 2)\n",
    "        :param known_one_hot: (batch_size, personal_node_num, feature_size)\n",
    "        :param unknown_one_hot: (batch_size, personal_node_num, feature_size)\n",
    "        :param known_differ_one_hot: (batch_size, personal_node_num, feature_size)\n",
    "        :param total_qa_turn_one_hot: (batch_size, feature_size)\n",
    "        :param personal_nodes: (batch_size, personal_node_num)\n",
    "        :param final_node_embed: (batch_size, max_node_num, feature_size)\n",
    "        :return: manager_state: (batch_size, feature_size)\n",
    "        \"\"\"\n",
    "        batch_size = personal_nodes.shape[0]\n",
    "        personal_node_num = personal_nodes.shape[1]\n",
    "\n",
    "        personal_node_embed = batch_embedding_lookup(final_node_embed, personal_nodes)\n",
    "        transferred_personal_node_embed = self.transfer(personal_node_embed)\n",
    "\n",
    "        if self.msg_agg == 'sum':\n",
    "            agg_state = torch.sum(transferred_personal_node_embed, 1)\n",
    "        elif self.msg_agg == 'avg':\n",
    "            agg_state = torch.sum(transferred_personal_node_embed, 1) / personal_node_num\n",
    "        elif self.msg_agg == 'max':\n",
    "            agg_state, _ = torch.max(transferred_personal_node_embed, 1)\n",
    "        else:\n",
    "            raise ValueError('Unknown message aggregation method')\n",
    "\n",
    "        manager_state = torch.cat((feasible_personal_info_nodes,\n",
    "                                   workers_decision.reshape(batch_size, -1),\n",
    "                                   known_one_hot.reshape(batch_size, -1),\n",
    "                                   unknown_one_hot.reshape(batch_size, -1),\n",
    "                                   known_differ_one_hot.reshape(batch_size, -1),\n",
    "                                   total_qa_turn_one_hot,\n",
    "                                   agg_state), 1)\n",
    "        return manager_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCPJw_hd-pU7"
   },
   "source": [
    "## **Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9uh4xc3y-k6y"
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = r'C:\\Users\\Mohamed Taha\\Desktop\\FinalWork\\data'\n",
    "\n",
    "# coding = utf-8\n",
    "import torch\n",
    "import os\n",
    "\n",
    "#DATA_ROOT = os.path.abspath(r'C:\\Users\\M.user\\Desktop\\translate\\translated')\n",
    "\n",
    "# for data set split\n",
    "train_size = 706\n",
    "test_size = 100\n",
    "dev_size = 100\n",
    "# for pad\n",
    "# NodePad depends on the valid num of nodes\n",
    "RelationPad = 0\n",
    "EdgePad = (0, 0, 0)\n",
    "\n",
    "PLACE_HOLDER = None\n",
    "\n",
    "# for manager and worker terminal actions, to keep in accordance to other actions\n",
    "# we insert [FRAUD, NON_FRAUD] in the tail of action space\n",
    "FRAUD = -2\n",
    "NON_FRAUD = -1\n",
    "\n",
    "# simulate user know or do not know about a triple\n",
    "Known = 1\n",
    "UnKnown = -1\n",
    "NotClear = 0\n",
    "ShowUnknown = \"Unknown\"\n",
    "UnKnownUtterance = \"我不清楚\"\n",
    "\n",
    "NegativeSampledAnswerNum = 2\n",
    "Options = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# for user type sample\n",
    "User_Type_Weights = {\"Type-4 Fraud\": 1,\n",
    "                     \"Type-3 Fraud\": 1,\n",
    "                     \"Type-2 Fraud\": 1,\n",
    "                     \"Type-1 Fraud\": 1,\n",
    "                     \"Non-Fraud\": 4}\n",
    "Personal_Information_Fraud_Weights = {\"company\": 2, \"university\": 2, \"live_in\": 1, \"born_in\": 1}\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "# Turn exploring time step constrain\n",
    "# Note: the exploring time steps are not the same as dialogue turns,\n",
    "# punish exploring time steps will punish dialogue turns, not vice versa\n",
    "MaxExploringTimeStep = 40\n",
    "MaxWorkerExploringTimeStep = 10\n",
    "MinFlattenRLQATurn = 8\n",
    "MaxFlattenRuleQATurn = 10\n",
    "\n",
    "# For rule based warm ups\n",
    "MinDifference = 3\n",
    "MinWorkerQATurn = 3\n",
    "\n",
    "# Rewards\n",
    "WorkerBonus = 0\n",
    "ManagerBonus = 0\n",
    "# give bonus in hrl model setting, if not, the rl will collapse, just a trick!\n",
    "# WorkerBonus = 2\n",
    "# ManagerBonus = 1.5\n",
    "ManagerRecognitionCorrect = 3\n",
    "ManagerRecognitionWrong = -3\n",
    "WorkerRecognitionCorrect = 1\n",
    "WorkerRecognitionWrong = -1\n",
    "ExploringPunish = -0.1\n",
    "WorkerRewardDiscount = 0.99\n",
    "ManagerRewardDiscount = 0.999\n",
    "\n",
    "# config for torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# for node feature\n",
    "PERSONAL_NODE_FILED = 4\n",
    "ONE_STEP_NODE_FILED = 1\n",
    "SE_FREQS_FILED = 10\n",
    "DEGREE_FILED = 10\n",
    "STATIC_FEATURE_SIZE = PERSONAL_NODE_FILED + ONE_STEP_NODE_FILED + SE_FREQS_FILED + DEGREE_FILED\n",
    "DYNAMIC_FEATURE_SIZE = 7\n",
    "Init_Node_Feature_Size = STATIC_FEATURE_SIZE + DYNAMIC_FEATURE_SIZE\n",
    "ManagerStateRest = 4 + 2 * 4 + (MaxWorkerExploringTimeStep + 1) * 3 * 4 + (MaxExploringTimeStep + 1)\n",
    "WorkersStateRest = (MaxWorkerExploringTimeStep + 1) * 3 + (MaxWorkerExploringTimeStep + 1) + (\n",
    "        MaxWorkerExploringTimeStep + 1)\n",
    "\n",
    "# for flatten rl, this node idx is the padded query node\n",
    "Pad_Query_Node = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8vBVm07-4Jh"
   },
   "source": [
    "# State Tracker:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Htt7vJiv-9tM"
   },
   "source": [
    "## Node *Feature*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DYPh-lQG-v4t"
   },
   "outputs": [],
   "source": [
    "# coding = utf-8\n",
    "from itertools import chain\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "#from src.config import DATA_ROOT, DEGREE_FILED, SE_FREQS_FILED, DYNAMIC_FEATURE_SIZE\n",
    "\n",
    "with open(os.path.join(DATA_ROOT, \"se_freqs_bins.json\"), \"r\") as f:\n",
    "    se_freqs_bins = json.load(f)\n",
    "\n",
    "\n",
    "class NodeFeature(object):\n",
    "    def __init__(self):\n",
    "        self.se_freqs_bins = se_freqs_bins\n",
    "\n",
    "    def static_feature(self,\n",
    "                       nodes,\n",
    "                       personal_information,\n",
    "                       one_step_nodes,\n",
    "                       node_se_freqs,\n",
    "                       node_degree,\n",
    "                       node_in_degree,\n",
    "                       node_out_degree):\n",
    "        \"\"\"  Get feature before the dialogue.  \"\"\"\n",
    "        answer_nodes = list(chain(*one_step_nodes.values()))\n",
    "        static_feature_matrix = list()\n",
    "        for node, se_freqs, degree, in_degree, out_degree in zip(nodes,\n",
    "                                                                 node_se_freqs,\n",
    "                                                                 node_degree,\n",
    "                                                                 node_in_degree,\n",
    "                                                                 node_out_degree):\n",
    "            vector = list()\n",
    "\n",
    "            # the personal information type of this node\n",
    "            for value in personal_information.values():\n",
    "                if node == value:\n",
    "                    vector.append(1)\n",
    "                else:\n",
    "                    vector.append(0)\n",
    "\n",
    "            # if the node is the answer node\n",
    "            if node in answer_nodes:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "\n",
    "            for i, max_se_freqs in enumerate(self.se_freqs_bins):\n",
    "                if math.log(se_freqs + 1) <= max_se_freqs:\n",
    "                    vector.extend(self.one_hot(i, SE_FREQS_FILED))\n",
    "                    break\n",
    "\n",
    "            vector.extend(self.one_hot(degree, DEGREE_FILED))\n",
    "            # vector.extend(self.one_hot(in_degree, IN_DEGREE_FILED))\n",
    "            # vector.extend(self.one_hot(out_degree, OUT_DEGREE_FILED))\n",
    "            static_feature_matrix.append(vector)\n",
    "        return static_feature_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot(idx, max_length): \n",
    "        v = [0 for _ in range(max_length)]\n",
    "        if idx > max_length - 1:\n",
    "            v[-1] = 1\n",
    "        else:\n",
    "            v[idx] = 1\n",
    "        return v\n",
    "\n",
    "    @staticmethod\n",
    "    def dialogue_feature(max_node_num,\n",
    "                         nodes,\n",
    "                         explored_nodes,\n",
    "                         last_turn_q_node,\n",
    "                         last_turn_a_node,\n",
    "                         not_explored_nodes,\n",
    "                         known_nodes,\n",
    "                         unknown_nodes,\n",
    "                         not_answered_nodes):\n",
    "        \"\"\"  Get feature during the dialogue. Support batch.  \"\"\"\n",
    "        dialogue_feature_matrix = np.zeros((max_node_num, DYNAMIC_FEATURE_SIZE))\n",
    "\n",
    "        for node in nodes:\n",
    "            vector = list()\n",
    "            if node in explored_nodes:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "\n",
    "            if node == last_turn_q_node:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "\n",
    "            if node == last_turn_a_node:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "\n",
    "            if node in not_explored_nodes:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "\n",
    "            if node in known_nodes:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "\n",
    "            if node in unknown_nodes:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "\n",
    "            if node in not_answered_nodes:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "\n",
    "            dialogue_feature_matrix[node] = np.asarray(vector, dtype=np.float32)\n",
    "\n",
    "        return dialogue_feature_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOMG3wh0_Eze"
   },
   "source": [
    "## State Tracker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ijQhGDnZE8ys"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(vec, size):\n",
    "    \"\"\"\n",
    "    :param vec: any shape vector\n",
    "    :param size: the one hot size\n",
    "    :return: one hot vector of tensor\n",
    "    \"\"\"\n",
    "    shape = vec.shape\n",
    "    vec_flat = vec.reshape(-1)\n",
    "\n",
    "    one_hot_vec = np.zeros((vec_flat.shape[0], size))\n",
    "    #try:\n",
    "    one_hot_vec[np.arange(vec_flat.shape[0]), vec_flat] = 1\n",
    "    #except Exception as e:\n",
    "      #print(vec_flat.shape[0], vec_flat)\n",
    "      #print('one hot error', e)\n",
    "    one_hot_vec = one_hot_vec.reshape((*shape, size))\n",
    "    return one_hot_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f0iQbRLW-8UJ"
   },
   "outputs": [],
   "source": [
    "# coding = utf-8\n",
    "import copy\n",
    "import numpy as np\n",
    "#from src.config import NON_FRAUD, FRAUD, Known, UnKnown, PLACE_HOLDER, MaxExploringTimeStep, \\\n",
    "    #MaxWorkerExploringTimeStep, ManagerRecognitionCorrect, ManagerRecognitionWrong, WorkerRecognitionCorrect, \\\n",
    "    #WorkerRecognitionWrong, ExploringPunish, MinDifference, MinWorkerQATurn, WorkerBonus, ManagerBonus, \\\n",
    "    #MaxFlattenRuleQATurn, MinFlattenRLQATurn\n",
    "#from src.Graph.node_feature import NodeFeature\n",
    "\n",
    "\n",
    "class StateTracker(object):\n",
    "    def __init__(self, state_tracker_field):\n",
    "        self.init_episode(state_tracker_field)\n",
    "\n",
    "    def generate_recent_dialogue_feature(self):\n",
    "        max_node_num = self.state_tracker_field[-1][\"max_node_num\"]\n",
    "        nodes = self.state_tracker_field[-1][\"nodes\"]\n",
    "        explored_nodes = self.state_tracker_field[-1][\"explored_nodes\"]\n",
    "        last_turn_q_node = self.state_tracker_field[-1][\"last_turn_q_node\"]\n",
    "        last_turn_a_node = self.state_tracker_field[-1][\"last_turn_a_node\"]\n",
    "        not_explored_nodes = self.state_tracker_field[-1][\"not_explored_nodes\"]\n",
    "        known_nodes = self.state_tracker_field[-1][\"known_nodes\"]\n",
    "        unknown_nodes = self.state_tracker_field[-1][\"unknown_nodes\"]\n",
    "        not_answered_nodes = self.state_tracker_field[-1][\"not_answered_nodes\"]\n",
    "        self.state_tracker_field[-1][\"dialogue_feature\"] = self.node_feat_generator.dialogue_feature(max_node_num,\n",
    "                                                                                                     nodes,\n",
    "                                                                                                     explored_nodes,\n",
    "                                                                                                     last_turn_q_node,\n",
    "                                                                                                     last_turn_a_node,\n",
    "                                                                                                     not_explored_nodes,\n",
    "                                                                                                     known_nodes,\n",
    "                                                                                                     unknown_nodes,\n",
    "                                                                                                     not_answered_nodes)\n",
    "\n",
    "    def init_episode(self, state_tracker_field):\n",
    "        self.state_tracker_field = state_tracker_field\n",
    "        self.node_feat_generator = NodeFeature()\n",
    "        self.generate_recent_dialogue_feature()\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "              \n",
    "\n",
    "class StateTrackerHRL(StateTracker):\n",
    "    def __init__(self, state_tracker_field):\n",
    "        super(StateTrackerHRL, self).__init__(state_tracker_field)\n",
    "    \n",
    "\n",
    "    # NEW FN:\n",
    "    def generate_system_action(self,\n",
    "                            manager_sample_idx,\n",
    "                            manager_sample_content,\n",
    "                            workers_sample_idx,\n",
    "                            workers_sample_content,\n",
    "                            #user_identity_state,\n",
    "                            #user_sub_identity_state_dict\n",
    "                            ):\n",
    "      policy_mask = self.state_tracker_field[-1][\"policy_mask\"]\n",
    "      policy_idx = policy_mask.argmax()\n",
    "\n",
    "      if policy_idx == 0:\n",
    "          system_action = {\"manager\": {\"sample_idx\": manager_sample_idx,\n",
    "                                        \"sample_content\": manager_sample_content}}\n",
    "\n",
    "          if manager_sample_content in [FRAUD, NON_FRAUD]:\n",
    "            self.state_tracker_field[-1][\"manager_decision\"] = manager_sample_content\n",
    "\n",
    "\n",
    "      else:\n",
    "        worker_idx = policy_idx - 1\n",
    "\n",
    "        system_action = {\"worker\": {\"sample_idx\": workers_sample_idx[worker_idx],\n",
    "                                    \"sample_content\": workers_sample_content[worker_idx]}}\n",
    "\n",
    "        if workers_sample_content[worker_idx] in [FRAUD, NON_FRAUD]:\n",
    "          self.state_tracker_field[-1][\"workers_decision\"][worker_idx] = workers_sample_content[worker_idx]\n",
    "      \n",
    "      self.state_tracker_field[-1][\"system_action\"] = system_action\n",
    "      \n",
    "      return system_action\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    def generate_workers_reward(self):\n",
    "        # call this function after the worker choose a terminal action\n",
    "        policy_mask = self.state_tracker_field[-1][\"policy_mask\"]\n",
    "        worker_idx = policy_mask.argmax() - 1\n",
    "\n",
    "        worker_decision = self.state_tracker_field[-1][\"workers_decision\"][worker_idx]\n",
    "        worker_success_state = self.state_tracker_field[-1][\"workers_success_state\"][worker_idx]\n",
    "\n",
    "        if worker_success_state is True:\n",
    "            if worker_decision == FRAUD:\n",
    "                self.state_tracker_field[-1][\"workers_reward\"] += (WorkerRecognitionCorrect + WorkerBonus)\n",
    "            else:\n",
    "                self.state_tracker_field[-1][\"workers_reward\"] += WorkerRecognitionCorrect\n",
    "        else:\n",
    "            if worker_decision != FRAUD:\n",
    "                self.state_tracker_field[-1][\"workers_reward\"] += (WorkerRecognitionWrong - WorkerBonus)\n",
    "            else:\n",
    "                self.state_tracker_field[-1][\"workers_reward\"] += WorkerRecognitionWrong\n",
    "\n",
    "        # rollback to give turn punishment to each worker action\n",
    "        for i in range(-2, -(len(self.state_tracker_field) + 1), -1):\n",
    "            if self.state_tracker_field[i][\"policy_mask\"].argmax() == 0:\n",
    "                break\n",
    "            self.state_tracker_field[i][\"workers_reward\"] += ExploringPunish\n",
    "\n",
    "    def _worker_detect_a_fraud(self):\n",
    "        flag = False\n",
    "        for worker_decision, worker_success_state in zip(self.state_tracker_field[-1][\"workers_decision\"],\n",
    "                                                         self.state_tracker_field[-1][\"workers_success_state\"]):\n",
    "            if worker_decision == FRAUD and worker_success_state is True:\n",
    "                flag = True\n",
    "                break\n",
    "        return flag\n",
    "\n",
    "    def _worker_detect_all_non_fraud(self):\n",
    "        flag = True\n",
    "        for worker_decision, worker_success_state in zip(self.state_tracker_field[-1][\"workers_decision\"],\n",
    "                                                         self.state_tracker_field[-1][\"workers_success_state\"]):\n",
    "            if not ((worker_decision == PLACE_HOLDER and worker_success_state == PLACE_HOLDER) or (\n",
    "                    worker_decision == NON_FRAUD and worker_success_state is True)):\n",
    "                flag = False\n",
    "                break\n",
    "        return flag\n",
    "\n",
    "    def generate_manager_reward(self):\n",
    "        # call this function after the manager execute an action\n",
    "        manager_decision = self.state_tracker_field[-1][\"manager_decision\"]\n",
    "        manager_success_state = self.state_tracker_field[-1][\"manager_success_state\"]\n",
    "\n",
    "        if manager_decision in [FRAUD, NON_FRAUD]:\n",
    "            if manager_success_state is True:\n",
    "                if not (self._worker_detect_a_fraud() or self._worker_detect_all_non_fraud()):\n",
    "                    self.state_tracker_field[-1][\"manager_reward\"] += (ManagerRecognitionCorrect + ManagerBonus)\n",
    "                else:\n",
    "                    self.state_tracker_field[-1][\"manager_reward\"] += ManagerRecognitionCorrect\n",
    "            else:\n",
    "                if self._worker_detect_a_fraud() or self._worker_detect_all_non_fraud():\n",
    "                    self.state_tracker_field[-1][\"manager_reward\"] += (ManagerRecognitionWrong - ManagerBonus)\n",
    "                else:\n",
    "                    self.state_tracker_field[-1][\"manager_reward\"] += ManagerRecognitionWrong\n",
    "\n",
    "        # rollback to give turn punishment to the last manager action\n",
    "        worker_exploring_time = self.state_tracker_field[-1][\"current_worker_exploring_turn\"]\n",
    "\n",
    "        if len(self.state_tracker_field) > 1:\n",
    "            for i in range(-2, -(len(self.state_tracker_field) + 1), -1):\n",
    "                if self.state_tracker_field[i][\"policy_mask\"].argmax() == 0:\n",
    "                    self.state_tracker_field[i][\"manager_reward\"] += worker_exploring_time * ExploringPunish\n",
    "                    break\n",
    "\n",
    "    def move_a_step(self,\n",
    "                    language_generator,\n",
    "                    manager_sample_idx,\n",
    "                    manager_sample_content,\n",
    "                    manager_action_prob,\n",
    "                    workers_sample_idx,\n",
    "                    workers_sample_content,\n",
    "                    workers_action_prob,\n",
    "                    mode):\n",
    "        \"\"\"\n",
    "        In current state S_{t}, the system execute the system action.\n",
    "        And we get the new dialogue state S_{t+1}.\n",
    "        Then, we generate all mask for the new state S_{t+1} based on dialogue context.\n",
    "        :param language_generator: LanguageGenerator Class\n",
    "        :param user: UserSimulator Class\n",
    "        :param dialogue_recorder: DialogueRecorder Class or None\n",
    "        :param manager_sample_idx:\n",
    "        :param manager_sample_content:\n",
    "        :param manager_action_prob:\n",
    "        :param workers_sample_idx:\n",
    "        :param workers_sample_content:\n",
    "        :param workers_action_prob:\n",
    "        :param mode: indicate rule based warm up or RL\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # just for debug\n",
    "        self.state_tracker_field[-1][\"manager_action_prob\"] = manager_action_prob\n",
    "        self.state_tracker_field[-1][\"workers_action_prob\"] = workers_action_prob\n",
    "\n",
    "        # pad to the longest episode\n",
    "        self.state_tracker_field[-1][\"manager_sample_idx\"] = manager_sample_idx\n",
    "        self.state_tracker_field[-1][\"workers_sample_idx\"] = workers_sample_idx\n",
    "\n",
    "        if self.state_tracker_field[-1][\"episode_not_end\"] is False:\n",
    "            print('self.state_tracker_field[-1][\"episode_not_end\"] is False')\n",
    "            # pad to the longest episode\n",
    "            self.state_tracker_field.append(copy.deepcopy(self.state_tracker_field[-1]))\n",
    "        else:\n",
    "            # generate the system action in the current valid dialogue state first  #### CHANGE\n",
    "            system_action = self.generate_system_action(manager_sample_idx, manager_sample_content,\n",
    "                                                        workers_sample_idx, workers_sample_content)\n",
    "            #print(system_action)\n",
    "            # and then inherit information of it\n",
    "            state_in_new_step = copy.deepcopy(self.state_tracker_field[-1])\n",
    "\n",
    "            state_in_new_step[\"total_exploring_turn\"] += 1\n",
    "            if state_in_new_step[\"total_exploring_turn\"] >= MaxExploringTimeStep:\n",
    "                # terminal hrl recursively in the next step by force\n",
    "                state_in_new_step[\"rl_manager_action_mask\"][:-2] = 0\n",
    "                state_in_new_step[\"rl_manager_action_mask\"][-2:] = 1\n",
    "                state_in_new_step[\"rl_workers_action_mask\"][:, :-2] = 0\n",
    "                state_in_new_step[\"rl_workers_action_mask\"][:, -2:] = 1\n",
    "\n",
    "            # generate question\n",
    "            q_node, a_node = language_generator.generate_question(system_action)\n",
    "            questions, choices  =  new_answer(q_node, a_node, language_generator)\n",
    "            \n",
    "            \n",
    "        return questions, choices , state_in_new_step , system_action , language_generator , a_node , q_node\n",
    "\n",
    "    def continue_a_step(self,\n",
    "#                     language_generator,\n",
    "#                     manager_sample_idx,\n",
    "#                     manager_sample_content,\n",
    "#                     manager_action_prob,\n",
    "#                     workers_sample_idx,\n",
    "#                     workers_sample_content,\n",
    "#                     workers_action_prob,\n",
    "#                     mode,\n",
    "                         user_answer,state_in_new_step,system_action,language_generator,choices, a_node,q_node):\n",
    "      \n",
    "#             add node2idx to graph a7sn\n",
    "        if (user_answer is not None ) and (choices is not None):\n",
    "\n",
    "            idx = list(language_generator.language_generation_filed['idx2node'].keys())\n",
    "            nodes = list(language_generator.language_generation_filed['idx2node'].values())\n",
    "\n",
    "            for sym, answer in choices:\n",
    "                if sym == user_answer:\n",
    "                    try:\n",
    "                        pos = nodes.index(answer)\n",
    "                        user_answer = int(idx[pos])\n",
    "\n",
    "                    except:\n",
    "                        user_answer = -1\n",
    "        if self.state_tracker_field[-1][\"episode_not_end\"] is True:            \n",
    "\n",
    "            if user_answer is not None:\n",
    "\n",
    "              state_in_new_step[\"total_qa_turn\"] += 1\n",
    "\n",
    "            state_in_new_step[\"dialogue_feature\"] = PLACE_HOLDER\n",
    "            state_in_new_step[\"manager_sample_idx\"] = PLACE_HOLDER\n",
    "            state_in_new_step[\"workers_sample_idx\"] = PLACE_HOLDER\n",
    "            state_in_new_step[\"system_action\"] = PLACE_HOLDER\n",
    "            state_in_new_step[\"reward_mask\"] = PLACE_HOLDER\n",
    "            state_in_new_step[\"manager_reward\"] = np.zeros((1,), dtype=np.float32)\n",
    "            state_in_new_step[\"workers_reward\"] = np.zeros((len(state_in_new_step[\"personal_nodes\"]),),\n",
    "                                                          dtype=np.float32)\n",
    "              \n",
    "            manager_action = system_action.get(\"manager\", {\"sample_idx\": None, \"sample_content\": None})\n",
    "            worker_action = system_action.get(\"worker\", {\"sample_idx\": None, \"sample_content\": None})\n",
    "\n",
    "\n",
    "            manager_action_idx = manager_action[\"sample_idx\"]\n",
    "            manager_action_content = manager_action[\"sample_content\"]\n",
    "            worker_action_idx = worker_action[\"sample_idx\"]\n",
    "            worker_action_content = worker_action[\"sample_content\"]\n",
    "\n",
    "\n",
    "            if manager_action_content in [FRAUD, NON_FRAUD]:\n",
    "              state_in_new_step[\"episode_not_end\"] = False\n",
    "              #print('ended' * 10)\n",
    "              if manager_action_content == FRAUD:\n",
    "                print('FRAUD !!!!!')\n",
    "                return 'fraud'\n",
    "                \n",
    "              else:\n",
    "                print('NON-Fraud')\n",
    "                return 'nonfraud'\n",
    "            \n",
    "            elif worker_action_idx is None:\n",
    "              state_in_new_step[\"valid_workers_num\"] += 1\n",
    "\n",
    "              state_in_new_step[\"current_worker_exploring_turn\"] = 0\n",
    "\n",
    "              # generate for dialogue feature\n",
    "              state_in_new_step[\"explored_nodes\"].add(manager_action_content)\n",
    "              state_in_new_step[\"last_turn_q_node\"] = None\n",
    "              state_in_new_step[\"last_turn_a_node\"] = None\n",
    "              if manager_action_content in state_in_new_step[\"not_explored_nodes\"]:\n",
    "                  state_in_new_step[\"not_explored_nodes\"].remove(manager_action_content)\n",
    "\n",
    "              state_in_new_step[\"rl_manager_action_mask\"][manager_action_idx] = 0\n",
    "\n",
    "              policy_mask = [0] + [0 for _ in state_in_new_step[\"personal_nodes\"]]\n",
    "              worker_idx = state_in_new_step[\"personal_nodes\"].index(manager_action_content)\n",
    "              policy_mask[worker_idx + 1] = 1\n",
    "              state_in_new_step[\"policy_mask\"] = np.asarray(policy_mask, dtype=np.int32)\n",
    "              #print('right' * 10)\n",
    "\n",
    "            elif user_answer is None:\n",
    "\n",
    "                if worker_action_content == FRAUD or state_in_new_step[\"rl_manager_action_mask\"].sum() == 0:\n",
    "                    state_in_new_step[\"rl_manager_action_mask\"][-2:] = 1\n",
    "\n",
    "                state_in_new_step[\"last_turn_q_node\"] = None\n",
    "                state_in_new_step[\"last_turn_a_node\"] = None\n",
    "\n",
    "                # get new policy mask\n",
    "                policy_mask = [1] + [0 for _ in state_in_new_step[\"personal_nodes\"]]\n",
    "                state_in_new_step[\"policy_mask\"] = np.asarray(policy_mask, dtype=np.int32)\n",
    "                #print('wrong 1' * 15)\n",
    "\n",
    "            else:\n",
    "                \"\"\"  low level explore (Worker)  \"\"\"\n",
    "                state_in_new_step[\"current_worker_exploring_turn\"] += 1\n",
    "\n",
    "                worker_idx = state_in_new_step[\"policy_mask\"].argmax() - 1\n",
    "                state_in_new_step[\"workers_qa_turn\"][worker_idx] += 1\n",
    "\n",
    "                # generate for dialogue feature\n",
    "                state_in_new_step[\"explored_nodes\"].add(worker_action_content)\n",
    "                if worker_action_content in state_in_new_step[\"not_explored_nodes\"]:\n",
    "                    state_in_new_step[\"not_explored_nodes\"].remove(worker_action_content)\n",
    "                if worker_action_content in state_in_new_step[\"not_answered_nodes\"]:\n",
    "                    state_in_new_step[\"not_answered_nodes\"].remove(worker_action_content)\n",
    "                if user_answer == a_node:\n",
    "                    print('right answer!!! ' ,user_answer)\n",
    "                    state_in_new_step[\"known_nodes\"].add(worker_action_content)\n",
    "                    state_in_new_step[\"workers_counter\"][worker_idx][\"Known\"] += 1\n",
    "                elif user_answer != a_node:\n",
    "                    print('wrong answer!!! ' ,user_answer )\n",
    "                    state_in_new_step[\"unknown_nodes\"].add(worker_action_content)\n",
    "                    state_in_new_step[\"workers_counter\"][worker_idx][\"UnKnown\"] += 1\n",
    "                \n",
    "                state_in_new_step[\"last_turn_q_node\"] = q_node\n",
    "                state_in_new_step[\"last_turn_a_node\"] = a_node\n",
    "                #print('wrong 2' *15)\n",
    "                state_in_new_step[\"rl_workers_action_mask\"][worker_idx, worker_action_idx] = 0\n",
    "                if state_in_new_step[\"rl_workers_action_mask\"][worker_idx].sum() == 0 or \\\n",
    "                        state_in_new_step[\"workers_qa_turn\"][worker_idx] >= MinWorkerQATurn:\n",
    "                    state_in_new_step[\"rl_workers_action_mask\"][worker_idx, -2:] = 1\n",
    "                if state_in_new_step[\"current_worker_exploring_turn\"] >= MaxWorkerExploringTimeStep:\n",
    "                    # terminal current worker in the next step by force\n",
    "                    state_in_new_step[\"rl_workers_action_mask\"][worker_idx, :-2] = 0\n",
    "\n",
    "\n",
    "            self.state_tracker_field.append(state_in_new_step)\n",
    "            self.generate_recent_dialogue_feature()\n",
    "            return 'continue'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGy2SaChCgri"
   },
   "source": [
    "# Build graph embed inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "G3rnRx6mCkRr"
   },
   "outputs": [],
   "source": [
    "def build_graph_embed_inputs(graph_embed_field, state_tracker_field, rollout=True):\n",
    "\n",
    "\n",
    "  edges = graph_embed_field[\"edges\"]\n",
    "  node_edges = graph_embed_field[\"node_edges\"]\n",
    "  node_edge_mask = graph_embed_field[\"node_edge_mask\"]\n",
    "  static_feature = graph_embed_field[\"static_feature\"]\n",
    "  dialogue_feature = np.concatenate([[field[-1][\"dialogue_feature\"]] for field in state_tracker_field])\n",
    "\n",
    "  initial_node_embed = np.concatenate([static_feature, dialogue_feature], axis=-1)\n",
    "  feed_dict = dict()\n",
    "  feed_dict[\"initial_node_embed\"] = torch.Tensor(initial_node_embed).to(device=device, dtype=torch.float)\n",
    "  feed_dict[\"edges\"] = torch.Tensor(edges).to(device=device, dtype=torch.long)\n",
    "  feed_dict[\"node_edges\"] = torch.Tensor(node_edges).to(device=device, dtype=torch.long)\n",
    "  feed_dict[\"node_edge_mask\"] = torch.Tensor(node_edge_mask).to(device=device, dtype=torch.uint8)\n",
    "  return feed_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtJqXwHDCqFw"
   },
   "source": [
    "# Build manager state tracker inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aNvKnLTECvWQ"
   },
   "outputs": [],
   "source": [
    "def build_graph_embed_inputs(graph_embed_field, state_tracker_field, rollout=True):\n",
    "\n",
    "\n",
    "  edges = graph_embed_field[\"edges\"]\n",
    "  node_edges = graph_embed_field[\"node_edges\"]\n",
    "  node_edge_mask = graph_embed_field[\"node_edge_mask\"]\n",
    "  static_feature = graph_embed_field[\"static_feature\"]\n",
    "  dialogue_feature = np.concatenate([[field[-1][\"dialogue_feature\"]] for field in state_tracker_field])\n",
    "\n",
    "  initial_node_embed = np.concatenate([static_feature, dialogue_feature], axis=-1)\n",
    "  feed_dict = dict()\n",
    "  feed_dict[\"initial_node_embed\"] = torch.Tensor(initial_node_embed).to(device=device, dtype=torch.float)\n",
    "  feed_dict[\"edges\"] = torch.Tensor(edges).to(device=device, dtype=torch.long)\n",
    "  feed_dict[\"node_edges\"] = torch.Tensor(node_edges).to(device=device, dtype=torch.long)\n",
    "  feed_dict[\"node_edge_mask\"] = torch.Tensor(node_edge_mask).to(device=device, dtype=torch.uint8)\n",
    "  return feed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uZIx9O2C4J8"
   },
   "source": [
    "# Build manager state tracker inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hHDI5lQ7C5dO"
   },
   "outputs": [],
   "source": [
    "def build_manager_state_tracker_inputs(graph_embed_field, final_node_embed, workers_decision, state_tracker_field,\n",
    "                                       rollout=True):\n",
    "  \n",
    "\n",
    "  feasible_personal_info_nodes = graph_embed_field[\"feasible_personal_info_nodes\"]\n",
    "  personal_nodes = graph_embed_field[\"personal_nodes\"]\n",
    "  known = np.concatenate(\n",
    "            [[np.concatenate([[item[\"Known\"]] for item in field[-1][\"workers_counter\"]])] for field in\n",
    "             state_tracker_field])\n",
    "  unknown = np.concatenate(\n",
    "            [[np.concatenate([[item[\"UnKnown\"]] for item in field[-1][\"workers_counter\"]])] for field in\n",
    "             state_tracker_field])\n",
    "  known_differ = np.concatenate(\n",
    "            [[np.concatenate(\n",
    "                [[(item[\"Known\"] - item[\"UnKnown\"]) if (item[\"Known\"] - item[\"UnKnown\"]) > 0 else 0] for item in\n",
    "                 field[-1][\"workers_counter\"]])] for field in state_tracker_field])\n",
    "  total_qa_turn = np.concatenate([[field[-1][\"total_qa_turn\"]] for field in state_tracker_field])\n",
    "\n",
    "  known_one_hot = to_one_hot(known, size=(MaxWorkerExploringTimeStep + 1))\n",
    "  unknown_one_hot = to_one_hot(unknown, size=(MaxWorkerExploringTimeStep + 1))\n",
    "  known_differ_one_hot = to_one_hot(known_differ, size=(MaxWorkerExploringTimeStep + 1))\n",
    "  total_qa_turn_one_hot = to_one_hot(total_qa_turn, size=(MaxExploringTimeStep + 1))\n",
    "\n",
    "  feed_dict = dict()\n",
    "  feed_dict[\"feasible_personal_info_nodes\"] = torch.Tensor(feasible_personal_info_nodes).to(device=device,\n",
    "                                                                                            dtype=torch.float)\n",
    "  feed_dict[\"workers_decision\"] = workers_decision\n",
    "  feed_dict[\"known_one_hot\"] = torch.Tensor(known_one_hot).to(device=device, dtype=torch.float)\n",
    "  feed_dict[\"unknown_one_hot\"] = torch.Tensor(unknown_one_hot).to(device=device, dtype=torch.float)\n",
    "  feed_dict[\"known_differ_one_hot\"] = torch.Tensor(known_differ_one_hot).to(device, dtype=torch.float)\n",
    "  feed_dict[\"total_qa_turn_one_hot\"] = torch.Tensor(total_qa_turn_one_hot).to(device, dtype=torch.float)\n",
    "  feed_dict[\"personal_nodes\"] = torch.Tensor(personal_nodes).to(device=device, dtype=torch.long)\n",
    "  feed_dict[\"final_node_embed\"] = final_node_embed\n",
    "  return feed_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x-rutW-DEPk"
   },
   "source": [
    "# Build worker state tracker inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1WxV_fgKDFjp"
   },
   "outputs": [],
   "source": [
    "def build_workers_state_tracker_inputs(state_tracker_field, policy_field, final_node_embed, rollout=True):\n",
    "\n",
    "  personal_nodes = policy_field[\"manager_actions\"]\n",
    "  workers_max_qa_turn = policy_field[\"workers_max_qa_turn\"]\n",
    "  known = np.concatenate(\n",
    "          [[np.concatenate([[item[\"Known\"]] for item in field[-1][\"workers_counter\"]])] for field in\n",
    "            state_tracker_field])\n",
    "  unknown = np.concatenate(\n",
    "          [[np.concatenate([[item[\"UnKnown\"]] for item in field[-1][\"workers_counter\"]])] for field in\n",
    "            state_tracker_field])\n",
    "  known_differ = np.concatenate(\n",
    "          [[np.concatenate(\n",
    "              [[(item[\"Known\"] - item[\"UnKnown\"]) if (item[\"Known\"] - item[\"UnKnown\"]) > 0 else 0] for item in\n",
    "                field[-1][\"workers_counter\"]])] for field in state_tracker_field])\n",
    "  workers_qa_turn = np.concatenate([[field[-1][\"workers_qa_turn\"]] for field in state_tracker_field])\n",
    "\n",
    "  known_one_hot = to_one_hot(known, size=(MaxWorkerExploringTimeStep + 1))\n",
    "  unknown_one_hot = to_one_hot(unknown, size=(MaxWorkerExploringTimeStep + 1))\n",
    "  known_differ_one_hot = to_one_hot(known_differ, size=(MaxWorkerExploringTimeStep + 1))\n",
    "  workers_qa_turn_one_hot = to_one_hot(workers_qa_turn, size=(MaxWorkerExploringTimeStep + 1))\n",
    "  workers_max_qa_turn_one_hot = to_one_hot(workers_max_qa_turn, size=(MaxWorkerExploringTimeStep + 1))\n",
    "\n",
    "  feed_dict = dict()\n",
    "  feed_dict[\"known_one_hot\"] = torch.Tensor(known_one_hot).to(device=device, dtype=torch.float)\n",
    "  feed_dict[\"unknown_one_hot\"] = torch.Tensor(unknown_one_hot).to(device=device, dtype=torch.float)\n",
    "  feed_dict[\"known_differ_one_hot\"] = torch.Tensor(known_differ_one_hot).to(device, dtype=torch.float)\n",
    "  feed_dict[\"workers_qa_turn_one_hot\"] = torch.Tensor(workers_qa_turn_one_hot).to(device, dtype=torch.float)\n",
    "  feed_dict[\"workers_max_qa_turn_one_hot\"] = torch.Tensor(workers_max_qa_turn_one_hot).to(device, dtype=torch.float)\n",
    "  feed_dict[\"personal_nodes\"] = torch.Tensor(personal_nodes).to(device=device, dtype=torch.long)\n",
    "  feed_dict[\"final_node_embed\"] = final_node_embed\n",
    "  return feed_dict\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VqEHPUpDqC7"
   },
   "source": [
    "# Build hierarchy action masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5ES2yilNDu8d"
   },
   "outputs": [],
   "source": [
    "def build_hierarchy_action_masks(state_tracker_field, mode, rollout=True):\n",
    "\n",
    "  rl_manager_action_mask = np.concatenate(\n",
    "            [[field[-1][\"rl_manager_action_mask\"]] for field in state_tracker_field])\n",
    "  rl_workers_action_mask = np.concatenate(\n",
    "            [[field[-1][\"rl_workers_action_mask\"]] for field in state_tracker_field])\n",
    "  if mode == \"RuleWarmUp\":\n",
    "        warm_up_manager_action_mask = np.concatenate(\n",
    "            [[field[-1][\"warm_up_manager_action_mask\"]] for field in state_tracker_field])\n",
    "        warm_up_workers_action_mask = np.concatenate(\n",
    "            [[field[-1][\"warm_up_workers_action_mask\"]] for field in state_tracker_field])\n",
    "        \n",
    "  feed_dict = dict()\n",
    "  feed_dict[\"rl_manager_action_mask\"] = torch.Tensor(rl_manager_action_mask).to(device=device, dtype=torch.uint8)\n",
    "  feed_dict[\"rl_workers_action_mask\"] = torch.Tensor(rl_workers_action_mask).to(device=device, dtype=torch.uint8)\n",
    "  if mode == \"RuleWarmUp\":\n",
    "      feed_dict[\"warm_up_manager_action_mask\"] = torch.Tensor(warm_up_manager_action_mask).to(device=device,\n",
    "                                                                                              dtype=torch.uint8)\n",
    "      feed_dict[\"warm_up_workers_action_mask\"] = torch.Tensor(warm_up_workers_action_mask).to(device=device,\n",
    "                                                                                              dtype=torch.uint8)\n",
    "\n",
    "  return feed_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT6JtdeUD6Ko"
   },
   "source": [
    "# build_hierarchy_manager_inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PeWqZW4yERIB"
   },
   "outputs": [],
   "source": [
    "def build_hierarchy_manager_inputs(manager_state, workers_state, policy_field, rollout=True):\n",
    "\n",
    "  personal_nodes = policy_field[\"manager_actions\"]\n",
    "  feed_dict = dict()\n",
    "  feed_dict[\"manager_state\"] = manager_state\n",
    "  feed_dict[\"workers_state\"] = workers_state\n",
    "  feed_dict[\"personal_nodes\"] = torch.Tensor(personal_nodes).to(device=device, dtype=torch.long)\n",
    "  return feed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAnfT5flEaEO"
   },
   "source": [
    "# build_workers_inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ODz-_eT4EfK4"
   },
   "outputs": [],
   "source": [
    "def build_workers_inputs(workers_state, policy_field, final_node_embed, rollout=True):\n",
    "\n",
    "  answer_nodes = policy_field[\"workers_actions\"]\n",
    "  feed_dict = dict()\n",
    "  feed_dict[\"workers_state\"] = workers_state\n",
    "  feed_dict[\"answer_nodes\"] = torch.Tensor(answer_nodes).to(device=device, dtype=torch.long)\n",
    "  feed_dict[\"graph_node_embedding\"] = final_node_embed\n",
    "  return feed_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyr7FoUG_WUL"
   },
   "source": [
    "# Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jKGiIN2J_kut"
   },
   "outputs": [],
   "source": [
    "def build_init_models():\n",
    "    #if self.args.new_node_emb_size_list is not None:\n",
    "       # node_emb_size_list = [Init_Node_Feature_Size] + self.args.new_node_emb_size_list\n",
    "    \n",
    "    node_emb_size_list = [Init_Node_Feature_Size] + [40, 50 ]\n",
    "    graph_node_emb_size = sum(node_emb_size_list)\n",
    "    manager_state_size = 100 + ManagerStateRest\n",
    "    models = dict()\n",
    "    models[\"gnn\"] = GNN(node_emb_size_list, 'max').to(device)\n",
    "    worker_state_size = graph_node_emb_size + WorkersStateRest\n",
    "    models[\"manager_state_tracker\"] = ManagerStateTracker(graph_node_emb_size, 100,\n",
    "                                                          'max').to(device)\n",
    "    models[\"workers_state_tracker\"] = WorkersStateTracker().to(device)\n",
    "    models[\"manager\"] = Manager('concat', manager_state_size, worker_state_size).to(device)\n",
    "    models[\"workers\"] = Workers('concat', worker_state_size, graph_node_emb_size).to(device)\n",
    "    \n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fbDaOOZY_m4v"
   },
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Mohamed Taha\\Desktop\\FinalWork\\data\\checkpoints\\ghrl\\RL\\epoch_201_success_0.85_turn_9.71953125'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BbwXYzxR_R8-",
    "outputId": "c8bb7759-6668-43bf-e8ac-96794a900841"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gnn': GNN(\n",
       "   (linear_cells): ModuleList(\n",
       "     (0): Linear(in_features=32, out_features=40, bias=True)\n",
       "     (1): Linear(in_features=40, out_features=50, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'manager_state_tracker': ManagerStateTracker(\n",
       "   (transfer): Sequential(\n",
       "     (0): Linear(in_features=122, out_features=100, bias=True)\n",
       "     (1): Tanh()\n",
       "   )\n",
       " ),\n",
       " 'workers_state_tracker': WorkersStateTracker(),\n",
       " 'manager': Manager(\n",
       "   (policy_network): Attn(\n",
       "     (attn): Sequential(\n",
       "       (0): Linear(in_features=462, out_features=231, bias=True)\n",
       "       (1): Tanh()\n",
       "       (2): Linear(in_features=231, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       "   (value_network): Sequential(\n",
       "     (0): Linear(in_features=285, out_features=142, bias=True)\n",
       "     (1): Tanh()\n",
       "     (2): Linear(in_features=142, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'workers': Workers(\n",
       "   (policy_networks): Attn(\n",
       "     (attn): Sequential(\n",
       "       (0): Linear(in_features=299, out_features=149, bias=True)\n",
       "       (1): Tanh()\n",
       "       (2): Linear(in_features=149, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       "   (value_network): Sequential(\n",
       "     (0): Linear(in_features=177, out_features=88, bias=True)\n",
       "     (1): Tanh()\n",
       "     (2): Linear(in_features=88, out_features=1, bias=True)\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = build_init_models()\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yjwgafOJ_YUG"
   },
   "outputs": [],
   "source": [
    "for model_name in models.keys():\n",
    "    model_path = os.path.join(path, model_name +  '.pkl')\n",
    "    models[model_name].load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_lPbuNOBEsr"
   },
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "pz9DDUMoAi5W"
   },
   "outputs": [],
   "source": [
    "def new_answer(q_node, a_node, language_generator):\n",
    "    if q_node is None and a_node is None:\n",
    "        print ('Decision step')\n",
    "        return  None, None\n",
    "    questions, choices = language_generator.generate_sys_nl(q_node, a_node)\n",
    "    # print(\"plz answer the following question:\")\n",
    "    # print(a)\n",
    "    # print(b)\n",
    "\n",
    "\n",
    "    #ans = input('Enter Answer Symbol: (A-B-C-D): '  )\n",
    "\n",
    "#     # add node2idx to graph a7sn\n",
    "#     idx = list(language_generator.language_generation_filed['idx2node'].keys())\n",
    "#     nodes = list(language_generator.language_generation_filed['idx2node'].values())\n",
    "\n",
    "#     for sym, answer in choices:\n",
    "#         if sym == ans:\n",
    "#             try:\n",
    "#                 pos = nodes.index(answer)\n",
    "#                 aa_node = idx[pos]\n",
    "\n",
    "#             except:\n",
    "#                 aa_node = -1\n",
    "\n",
    "\n",
    "    return questions, choices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CspMSGJsBQbr"
   },
   "source": [
    "# Graph Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Br9cgTTGBJWU"
   },
   "outputs": [],
   "source": [
    "class GraphPreprocess(object):\n",
    "    def __init__(self, data_set_dir=\"preprocessed_graphs\", batch_size=32):\n",
    "        # preprocess data set\n",
    "        self.batch_size = batch_size\n",
    "        self.train_set = self.split_batch(data_set_dir, \"test.json\", batch_size)\n",
    "        self.test_set = self.split_batch(data_set_dir, \"test.json\", batch_size)\n",
    "        self.dev_set = self.split_batch(data_set_dir, \"test.json\", batch_size)\n",
    "\n",
    "    def split_batch(self, data_dir, data_file, batch_size):\n",
    "        with open(os.path.join(DATA_ROOT, data_dir, data_file), \"r\", encoding = 'utf-8') as f:\n",
    "            data_set = json.load(f)\n",
    "\n",
    "        for _ in range(batch_size - len(data_set) % batch_size):\n",
    "            data_set.append(copy.deepcopy(random.choice(data_set)))\n",
    "        data_set.sort(key=lambda graph: len(graph[\"edges\"]))\n",
    "\n",
    "        split_data_set = list()\n",
    "        for batch_idx in range(len(data_set) // batch_size):\n",
    "            batch = list()\n",
    "            for i in range(batch_size):\n",
    "                batch.append(data_set[batch_idx * batch_size + i])\n",
    "            split_data_set.append(self.preprocess_batch(batch))\n",
    "        return split_data_set\n",
    "\n",
    "    @staticmethod\n",
    "    def get_graph_embed_filed(batch):\n",
    "        \"\"\"  Used in Graph Embed  \"\"\"\n",
    "        max_node_num = 0\n",
    "        max_edge_num = 0\n",
    "        max_node_edge_num = 0\n",
    "        max_answer_node_num = 0\n",
    "        max_qa_pair_num = 0\n",
    "        for graph in batch:\n",
    "            current_node_num = len(graph[\"nodes\"])\n",
    "            current_edge_num = len(graph[\"edges\"])\n",
    "            current_max_node_edge_num = max(map(len, graph[\"node_edges\"]))\n",
    "            current_max_answer_node_num = max(map(len, list(graph[\"one_step_nodes\"].values())))\n",
    "            current_qa_pair_num = sum(map(len, list(graph[\"one_step_nodes\"].values())))\n",
    "\n",
    "            if max_node_num < current_node_num:\n",
    "                max_node_num = current_node_num\n",
    "            if max_edge_num < current_edge_num:\n",
    "                max_edge_num = current_edge_num\n",
    "            if max_node_edge_num < current_max_node_edge_num:\n",
    "                max_node_edge_num = current_max_node_edge_num\n",
    "            if max_answer_node_num < current_max_answer_node_num:\n",
    "                max_answer_node_num = current_max_answer_node_num\n",
    "            if max_qa_pair_num < current_qa_pair_num:\n",
    "                max_qa_pair_num = current_qa_pair_num\n",
    "\n",
    "        # add 1 for node pad\n",
    "        # the padded node idx is the current_node_num\n",
    "        # but pad with range(current_node_num, max_node_num)\n",
    "        max_node_num += 1\n",
    "\n",
    "        # add 1 for edge pad\n",
    "        # the padded edge idx is the current_edge_num\n",
    "        # but pad with EdgePad\n",
    "        max_edge_num += 1\n",
    "\n",
    "        batch_node_num = list()\n",
    "        batch_qa_pair_num = list()\n",
    "        batch_personal_nodes = list()\n",
    "        batch_feasible_personal_info_nodes = list()\n",
    "        batch_static_feature = list()\n",
    "        batch_edges = list()\n",
    "        batch_node_edge_mask = list()\n",
    "        batch_node_edges = list()\n",
    "\n",
    "        for graph in batch:\n",
    "            # **************** personal_nodes and node static feature ****************\n",
    "            current_node_num = len(graph[\"nodes\"])\n",
    "            batch_node_num.append(current_node_num)\n",
    "            current_qa_pair_num = sum(map(len, list(graph[\"one_step_nodes\"].values())))\n",
    "            batch_qa_pair_num.append(current_qa_pair_num)\n",
    "\n",
    "            personal_nodes = np.asarray(graph[\"personal_nodes\"], dtype=np.int32)\n",
    "            batch_personal_nodes.append(personal_nodes)\n",
    "\n",
    "            feasible_personal_info_nodes = np.zeros((len(graph[\"personal_nodes\"]),), dtype=np.float32)\n",
    "            for worker_idx, answer_nodes in graph[\"one_step_nodes\"].items():\n",
    "                if len(answer_nodes) > 0:\n",
    "                    feasible_personal_info_nodes[graph[\"personal_nodes\"].index(int(worker_idx))] = 1\n",
    "            batch_feasible_personal_info_nodes.append(feasible_personal_info_nodes)\n",
    "\n",
    "            original_static_feature = np.asarray(graph[\"static_feature\"], dtype=np.float32)\n",
    "            static_feature = np.zeros((max_node_num, STATIC_FEATURE_SIZE), dtype=np.float32)\n",
    "            static_feature[:original_static_feature.shape[0]] = original_static_feature\n",
    "            batch_static_feature.append(static_feature)\n",
    "\n",
    "            # **************** edges ****************\n",
    "            edges = np.zeros((max_edge_num, 3), dtype=np.int32)\n",
    "            current_edge_num = len(graph[\"edges\"])\n",
    "            edges[:current_edge_num] = np.asarray(graph[\"edges\"], dtype=np.int32)\n",
    "            edges[current_edge_num:] = np.tile(np.asarray(EdgePad, dtype=np.int32),\n",
    "                                               (max_edge_num - current_edge_num, 1))\n",
    "            batch_edges.append(edges)\n",
    "\n",
    "            # **************** pad node_edges and get mask ****************\n",
    "            current_node_edge_num = map(len, graph[\"node_edges\"])\n",
    "            node_edge_mask = np.zeros((max_node_num, max_node_edge_num), dtype=np.int32)\n",
    "            for i, num in enumerate(current_node_edge_num):\n",
    "                node_edge_mask[i, :num] = 1\n",
    "            batch_node_edge_mask.append(node_edge_mask)\n",
    "\n",
    "            node_edges = np.full((max_node_num, max_node_edge_num), current_edge_num, dtype=np.int32)\n",
    "            for i, x in enumerate(graph[\"node_edges\"]):\n",
    "                node_edges[i, :len(x)] = np.asarray(x, dtype=np.int32)\n",
    "            batch_node_edges.append(node_edges)\n",
    "\n",
    "        graph_embed_field = dict()\n",
    "        graph_embed_field[\"personal_nodes\"] = np.concatenate(\n",
    "            [[personal_nodes] for personal_nodes in batch_personal_nodes])\n",
    "        graph_embed_field[\"feasible_personal_info_nodes\"] = np.concatenate(\n",
    "            [[feasible_personal_info_nodes] for feasible_personal_info_nodes in batch_feasible_personal_info_nodes])\n",
    "        graph_embed_field[\"static_feature\"] = np.concatenate(\n",
    "            [[static_feature] for static_feature in batch_static_feature])\n",
    "        graph_embed_field[\"edges\"] = np.concatenate([[edges] for edges in batch_edges])\n",
    "        graph_embed_field[\"node_edge_mask\"] = np.concatenate(\n",
    "            [[node_edge_mask] for node_edge_mask in batch_node_edge_mask])\n",
    "        graph_embed_field[\"node_edges\"] = np.concatenate([[node_edges] for node_edges in batch_node_edges])\n",
    "\n",
    "        return graph_embed_field, batch_node_num, max_node_num, max_answer_node_num, max_qa_pair_num, batch_qa_pair_num\n",
    "\n",
    "    @staticmethod\n",
    "    def get_knowledge_sampler_field(batch):\n",
    "        \"\"\"  Used in User Simulators  \"\"\"\n",
    "        batch_knowledge_sampler_filed = list()\n",
    "        for graph in batch:\n",
    "            personal_information = copy.deepcopy(graph[\"personal_information\"])\n",
    "            one_step_node_edges = copy.deepcopy(graph[\"one_step_node_edges\"])\n",
    "            adj_matrix = np.asarray(graph[\"adj_matrix\"], dtype=bool)\n",
    "            edge_se_freqs_matrix = np.asarray(graph[\"edge_se_freqs_matrix\"], dtype=np.float32)\n",
    "            edges = copy.deepcopy(graph[\"edges\"])\n",
    "            identity_dict_keys = list(graph[\"one_step_node_edges\"].keys())\n",
    "            idx2node = copy.deepcopy(graph[\"idx2node\"])\n",
    "            knowledge_sampler_filed = dict()\n",
    "            knowledge_sampler_filed[\"personal_information\"] = personal_information\n",
    "            knowledge_sampler_filed[\"one_step_node_edges\"] = one_step_node_edges\n",
    "            knowledge_sampler_filed[\"adj_matrix\"] = adj_matrix\n",
    "            knowledge_sampler_filed[\"edge_se_freqs_matrix\"] = edge_se_freqs_matrix\n",
    "            knowledge_sampler_filed[\"edges\"] = edges\n",
    "            knowledge_sampler_filed[\"identity_dict_keys\"] = identity_dict_keys\n",
    "            knowledge_sampler_filed[\"idx2node\"] = idx2node\n",
    "            knowledge_sampler_filed[\"identity_dict_values\"] = PLACE_HOLDER\n",
    "            knowledge_sampler_filed[\"results\"] = PLACE_HOLDER\n",
    "            batch_knowledge_sampler_filed.append(knowledge_sampler_filed)\n",
    "        return batch_knowledge_sampler_filed\n",
    "\n",
    "    @staticmethod\n",
    "    def get_language_generation_field(batch):\n",
    "        batch_language_generation_filed = list()\n",
    "        for graph in batch:\n",
    "            language_generation_filed = dict()\n",
    "            language_generation_filed[\"idx2node\"] = copy.deepcopy(graph[\"idx2node\"])\n",
    "            h_t_to_r = dict()\n",
    "            for edge in graph[\"edges\"]:\n",
    "                h = edge[2]\n",
    "                t = edge[0]\n",
    "                r = edge[1]\n",
    "                key = str(h) + \" \" + str(t)\n",
    "                value = str(r)\n",
    "                h_t_to_r[key] = value\n",
    "            language_generation_filed[\"h_t_to_r\"] = h_t_to_r\n",
    "            batch_language_generation_filed.append(language_generation_filed)\n",
    "        return batch_language_generation_filed\n",
    "\n",
    "    def generator(self, data_set_name, shuffle=True):\n",
    "        if data_set_name == \"train\":\n",
    "            data_set = self.train_set\n",
    "        elif data_set_name == \"dev\":\n",
    "            data_set = self.dev_set\n",
    "        else:\n",
    "            data_set = self.test_set\n",
    "\n",
    "        size = len(data_set)\n",
    "        data_set_idx = list(range(size))\n",
    "        if shuffle:\n",
    "            random.shuffle(data_set_idx)\n",
    "\n",
    "        for idx in data_set_idx:\n",
    "            yield {\"graph_embed_field\": data_set[idx][\"graph_embed_field\"],\n",
    "                   \"policy_field\": data_set[idx][\"policy_field\"],\n",
    "                   \"knowledge_sampler_filed\": copy.deepcopy(data_set[idx][\"knowledge_sampler_filed\"]),\n",
    "                   \"language_generation_filed\": data_set[idx][\"language_generation_filed\"],\n",
    "                   \"state_tracker_field\": copy.deepcopy(data_set[idx][\"state_tracker_field\"])}\n",
    "\n",
    "\n",
    "class GraphPreprocessHRL(GraphPreprocess):\n",
    "    def __init__(self, data_set_dir=\"preprocessed_graphs\", batch_size=32):\n",
    "        super(GraphPreprocessHRL, self).__init__(data_set_dir, batch_size)\n",
    "\n",
    "    def preprocess_batch(self, batch):\n",
    "        graph_embed_field, batch_node_num, max_node_num, max_answer_node_num, _, _ = self.get_graph_embed_filed(batch)\n",
    "        policy_field = self.get_policy_field(batch, batch_node_num, max_answer_node_num)\n",
    "        knowledge_sampler_filed = self.get_knowledge_sampler_field(batch)\n",
    "        language_generation_filed = self.get_language_generation_field(batch)\n",
    "        state_tracker_field = self.get_state_tracker_field(batch, max_node_num, max_answer_node_num)\n",
    "        preprocessed_batch = {\"graph_embed_field\": graph_embed_field,\n",
    "                              \"policy_field\": policy_field,\n",
    "                              \"knowledge_sampler_filed\": knowledge_sampler_filed,\n",
    "                              \"language_generation_filed\": language_generation_filed,\n",
    "                              \"state_tracker_field\": state_tracker_field}\n",
    "        return preprocessed_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def get_state_tracker_field(batch, max_node_num, max_answer_node_num):\n",
    "        \"\"\"  A state tracker for each time step of a dialogue.\n",
    "             It records current state,\n",
    "             the mask information,\n",
    "             the sample information of manager and workers,\n",
    "             execute an action based on the state and get some reward  \"\"\"\n",
    "        batch_state_tracker_field = list()\n",
    "        for graph in batch:\n",
    "            # store state information of each time step in a list\n",
    "            state_tracker_field = list()\n",
    "\n",
    "            # this is the initial dialogue state\n",
    "            # for each time step, we generate a same data structure and append it.\n",
    "            state_in_initial_step = dict()\n",
    "\n",
    "            # some constant\n",
    "            # used in generate dialogue feature\n",
    "            state_in_initial_step[\"max_node_num\"] = max_node_num\n",
    "            state_in_initial_step[\"nodes\"] = copy.deepcopy(graph[\"nodes\"])\n",
    "            # used in move a new step\n",
    "            state_in_initial_step[\"personal_nodes\"] = copy.deepcopy(graph[\"personal_nodes\"])\n",
    "\n",
    "            # for all nodes\n",
    "            state_in_initial_step[\"explored_nodes\"] = set()\n",
    "            state_in_initial_step[\"last_turn_q_node\"] = None\n",
    "            state_in_initial_step[\"last_turn_a_node\"] = None\n",
    "            state_in_initial_step[\"not_explored_nodes\"] = set(copy.deepcopy(graph[\"nodes\"]))\n",
    "\n",
    "            # only for the answer nodes\n",
    "            state_in_initial_step[\"known_nodes\"] = set()\n",
    "            state_in_initial_step[\"unknown_nodes\"] = set()\n",
    "            state_in_initial_step[\"not_answered_nodes\"] = set(\n",
    "                list(chain(*graph[\"one_step_nodes\"].values())))\n",
    "\n",
    "            # dialogue feature of current state, calculate before all NN operations\n",
    "            state_in_initial_step[\"dialogue_feature\"] = PLACE_HOLDER\n",
    "\n",
    "            # episode end flag\n",
    "            state_in_initial_step[\"episode_not_end\"] = True\n",
    "\n",
    "            # exploring turn counter\n",
    "            state_in_initial_step[\"total_exploring_turn\"] = 0\n",
    "            state_in_initial_step[\"current_worker_exploring_turn\"] = 0\n",
    "\n",
    "            # policy mask, mark the running policy of current state\n",
    "            policy_mask = [1] + [0 for _ in graph[\"personal_nodes\"]]\n",
    "            state_in_initial_step[\"policy_mask\"] = np.asarray(policy_mask, dtype=np.int32)\n",
    "\n",
    "            # manager action mask\n",
    "            # create two counterparts, one for RL, one for RuleWarmUp\n",
    "            manager_action_mask = [0 for _ in graph[\"personal_nodes\"]]\n",
    "            for worker_idx, answer_nodes in graph[\"one_step_nodes\"].items():\n",
    "                if len(answer_nodes) > 0:\n",
    "                    manager_action_mask[graph[\"personal_nodes\"].index(int(worker_idx))] = 1\n",
    "            # append [0, 0] for two terminal actions\n",
    "            manager_action_mask += [0, 0]\n",
    "            state_in_initial_step[\"rl_manager_action_mask\"] = np.asarray(manager_action_mask, dtype=np.int32)\n",
    "            state_in_initial_step[\"warm_up_manager_action_mask\"] = np.asarray(manager_action_mask, dtype=np.int32)\n",
    "\n",
    "            # the workers and manager decision and if they are right\n",
    "            # used to get the reward Bonus\n",
    "            state_in_initial_step[\"workers_decision\"] = [PLACE_HOLDER for _ in graph[\"personal_nodes\"]]\n",
    "            state_in_initial_step[\"workers_success_state\"] = [PLACE_HOLDER for _ in graph[\"personal_nodes\"]]\n",
    "            state_in_initial_step[\"manager_decision\"] = PLACE_HOLDER\n",
    "            state_in_initial_step[\"manager_success_state\"] = PLACE_HOLDER\n",
    "\n",
    "            # worker action mask\n",
    "            # similar to manager action mask, the terminal action is available only after giving a few questions\n",
    "            # create two counterparts, one for RL, one for RuleWarmUp\n",
    "            workers_action_mask = np.zeros((len(graph[\"personal_nodes\"]), max_answer_node_num + 2), dtype=np.int32)\n",
    "            answer_node_num = list()\n",
    "            for worker in graph[\"personal_nodes\"]:\n",
    "                answer_node_num.append(len(graph[\"one_step_nodes\"][str(worker)]))\n",
    "            valid_worker_action_idx = list()\n",
    "            row, col = workers_action_mask.shape\n",
    "            for i, num in enumerate(answer_node_num):\n",
    "                for idx in range(num):\n",
    "                    valid_worker_action_idx.append(idx + i * col)\n",
    "            workers_action_mask = workers_action_mask.reshape(-1)\n",
    "            workers_action_mask[valid_worker_action_idx] = 1\n",
    "            workers_action_mask = workers_action_mask.reshape((row, col))\n",
    "            state_in_initial_step[\"rl_workers_action_mask\"] = workers_action_mask\n",
    "            state_in_initial_step[\"warm_up_workers_action_mask\"] = workers_action_mask.copy()\n",
    "\n",
    "            # the manager sample action idx, used for NN update\n",
    "            state_in_initial_step[\"manager_sample_idx\"] = PLACE_HOLDER\n",
    "\n",
    "            # the workers sample action idx, used for NN update\n",
    "            state_in_initial_step[\"workers_sample_idx\"] = PLACE_HOLDER\n",
    "\n",
    "            # the system action execute in current state\n",
    "            state_in_initial_step[\"system_action\"] = PLACE_HOLDER\n",
    "\n",
    "            # reward recorder, include reward mask, manager reward, workers reward\n",
    "            state_in_initial_step[\"reward_mask\"] = np.copy(state_in_initial_step[\"policy_mask\"])\n",
    "            state_in_initial_step[\"manager_reward\"] = np.zeros((1,), dtype=np.float32)\n",
    "            state_in_initial_step[\"workers_reward\"] = np.zeros((len(state_in_initial_step[\"personal_nodes\"]),),\n",
    "                                                               dtype=np.float32)\n",
    "\n",
    "            # qa statistical information\n",
    "            state_in_initial_step[\"total_qa_turn\"] = 0\n",
    "\n",
    "            # workers qa statistical information\n",
    "            state_in_initial_step[\"workers_qa_turn\"] = np.zeros((len(graph[\"personal_nodes\"]),), dtype=np.int32)\n",
    "\n",
    "            # debug\n",
    "            state_in_initial_step[\"manager_action_prob\"] = PLACE_HOLDER\n",
    "            state_in_initial_step[\"workers_action_prob\"] = PLACE_HOLDER\n",
    "\n",
    "            # for Rule based warm up\n",
    "            state_in_initial_step[\"workers_counter\"] = [{\"Known\": 0, \"UnKnown\": 0},\n",
    "                                                        {\"Known\": 0, \"UnKnown\": 0},\n",
    "                                                        {\"Known\": 0, \"UnKnown\": 0},\n",
    "                                                        {\"Known\": 0, \"UnKnown\": 0}]\n",
    "\n",
    "            # for divide in loss\n",
    "            state_in_initial_step[\"valid_workers_num\"] = 0\n",
    "\n",
    "            state_tracker_field.append(state_in_initial_step)\n",
    "            batch_state_tracker_field.append(state_tracker_field)\n",
    "\n",
    "        return batch_state_tracker_field\n",
    "\n",
    "    @staticmethod\n",
    "    def get_policy_field(batch, batch_node_num, max_answer_node_num):\n",
    "        batch_manager_actions = list()\n",
    "        batch_workers_actions = list()\n",
    "        batch_workers_max_qa_turn = list()\n",
    "\n",
    "        for graph, current_node_num in zip(batch, batch_node_num):\n",
    "            manager_actions = np.asarray(graph[\"personal_nodes\"], dtype=np.int32)\n",
    "            batch_manager_actions.append(manager_actions)\n",
    "\n",
    "            workers_actions = np.full((len(graph[\"personal_nodes\"]), max_answer_node_num), current_node_num,\n",
    "                                      dtype=np.int32)\n",
    "            workers_max_qa_turn = np.zeros((len(graph[\"personal_nodes\"]),), dtype=np.int32)\n",
    "            for worker_idx, answer_nodes in graph[\"one_step_nodes\"].items():\n",
    "                if len(answer_nodes) > 0:\n",
    "                    workers_actions[int(worker_idx), :len(answer_nodes)] = np.asarray(answer_nodes, dtype=np.int32)\n",
    "                    workers_max_qa_turn[int(worker_idx)] = len(answer_nodes) if len(\n",
    "                        answer_nodes) < MaxWorkerExploringTimeStep else MaxWorkerExploringTimeStep\n",
    "\n",
    "            batch_workers_actions.append(workers_actions)\n",
    "            batch_workers_max_qa_turn.append(workers_max_qa_turn)\n",
    "\n",
    "        policy_field = dict()\n",
    "        policy_field[\"manager_actions\"] = np.concatenate(\n",
    "            [[manager_actions] for manager_actions in batch_manager_actions])\n",
    "        policy_field[\"workers_actions\"] = np.concatenate(\n",
    "            [[workers_actions] for workers_actions in batch_workers_actions])\n",
    "        policy_field[\"workers_max_qa_turn\"] = np.concatenate(\n",
    "            [[workers_max_qa_turn] for workers_max_qa_turn in batch_workers_max_qa_turn])\n",
    "\n",
    "        return policy_field\n",
    "\n",
    "\n",
    "class GraphPreprocessRL(GraphPreprocess):\n",
    "    def __init__(self, data_set_dir=\"preprocessed_graphs\", batch_size=32):\n",
    "        super(GraphPreprocessRL, self).__init__(data_set_dir, batch_size)\n",
    "\n",
    "    def preprocess_batch(self, batch):\n",
    "        graph_embed_field, batch_node_num, max_node_num, _, max_qa_pair_num, batch_qa_pair_num = self.get_graph_embed_filed(\n",
    "            batch)\n",
    "        policy_field = self.get_policy_field(batch, batch_node_num, max_qa_pair_num)\n",
    "        knowledge_sampler_filed = self.get_knowledge_sampler_field(batch)\n",
    "        language_generation_filed = self.get_language_generation_field(batch)\n",
    "        state_tracker_field = self.get_state_tracker_field(batch, max_node_num, max_qa_pair_num, batch_qa_pair_num)\n",
    "        preprocessed_batch = {\"graph_embed_field\": graph_embed_field,\n",
    "                              \"policy_field\": policy_field,\n",
    "                              \"knowledge_sampler_filed\": knowledge_sampler_filed,\n",
    "                              \"language_generation_filed\": language_generation_filed,\n",
    "                              \"state_tracker_field\": state_tracker_field}\n",
    "        return preprocessed_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def get_policy_field(batch, batch_node_num, max_qa_pair_num):\n",
    "        batch_actions = list()\n",
    "        batch_workers_max_qa_turn = list()\n",
    "\n",
    "        for graph, current_node_num in zip(batch, batch_node_num):\n",
    "            actions = list()\n",
    "            for q_node, answer_nodes in graph[\"one_step_nodes\"].items():\n",
    "                for answer_node in answer_nodes:\n",
    "                    actions.append((int(q_node), int(answer_node)))\n",
    "            for _ in range(max_qa_pair_num - len(actions)):\n",
    "                actions.append((int(current_node_num), int(current_node_num)))\n",
    "            batch_actions.append(actions)\n",
    "\n",
    "            workers_max_qa_turn = np.zeros((len(graph[\"personal_nodes\"]),), dtype=np.int32)\n",
    "            batch_workers_max_qa_turn.append(workers_max_qa_turn)\n",
    "\n",
    "        policy_field = dict()\n",
    "        policy_field[\"actions\"] = np.concatenate(\n",
    "            [[actions] for actions in batch_actions])\n",
    "        policy_field[\"workers_max_qa_turn\"] = np.concatenate(\n",
    "            [[workers_max_qa_turn] for workers_max_qa_turn in batch_workers_max_qa_turn])\n",
    "\n",
    "        return policy_field\n",
    "\n",
    "    @staticmethod\n",
    "    def get_state_tracker_field(batch, max_node_num, max_qa_pair_num, batch_qa_pair_num):\n",
    "        batch_state_tracker_field = list()\n",
    "        for graph, qa_pair_num in zip(batch, batch_qa_pair_num):\n",
    "            state_tracker_field = list()\n",
    "            state_in_initial_step = dict()\n",
    "\n",
    "            state_in_initial_step[\"max_node_num\"] = max_node_num\n",
    "            state_in_initial_step[\"nodes\"] = copy.deepcopy(graph[\"nodes\"])\n",
    "\n",
    "            # for all nodes\n",
    "            state_in_initial_step[\"explored_nodes\"] = set()\n",
    "            state_in_initial_step[\"last_turn_q_node\"] = None\n",
    "            state_in_initial_step[\"last_turn_a_node\"] = None\n",
    "            state_in_initial_step[\"not_explored_nodes\"] = set(copy.deepcopy(graph[\"nodes\"]))\n",
    "\n",
    "            # only for the answer nodes\n",
    "            state_in_initial_step[\"known_nodes\"] = set()\n",
    "            state_in_initial_step[\"unknown_nodes\"] = set()\n",
    "            state_in_initial_step[\"not_answered_nodes\"] = set(\n",
    "                list(chain(*graph[\"one_step_nodes\"].values())))\n",
    "\n",
    "            # dialogue feature of current state, calculate before all NN operations\n",
    "            state_in_initial_step[\"dialogue_feature\"] = PLACE_HOLDER\n",
    "\n",
    "            # episode end flag\n",
    "            state_in_initial_step[\"episode_not_end\"] = True\n",
    "\n",
    "            # exploring turn counter\n",
    "            state_in_initial_step[\"total_exploring_turn\"] = 0\n",
    "\n",
    "            # rl action mask\n",
    "            rl_action_mask = np.zeros((max_qa_pair_num + 2,), dtype=np.int32)\n",
    "            rl_action_mask[:qa_pair_num] = 1\n",
    "            state_in_initial_step[\"rl_action_mask\"] = rl_action_mask\n",
    "            state_in_initial_step[\"warm_up_action_mask\"] = rl_action_mask.copy()\n",
    "\n",
    "            state_in_initial_step[\"decision\"] = PLACE_HOLDER\n",
    "            state_in_initial_step[\"success_state\"] = PLACE_HOLDER\n",
    "\n",
    "            state_in_initial_step[\"sample_idx\"] = PLACE_HOLDER\n",
    "\n",
    "            state_in_initial_step[\"system_action\"] = PLACE_HOLDER\n",
    "\n",
    "            state_in_initial_step[\"reward\"] = np.zeros((), dtype=np.float32)\n",
    "\n",
    "            state_in_initial_step[\"total_qa_turn\"] = 0\n",
    "\n",
    "            # debug\n",
    "            state_in_initial_step[\"action_prob\"] = PLACE_HOLDER\n",
    "\n",
    "            # for Rule based warm up\n",
    "            state_in_initial_step[\"workers_counter\"] = [{\"Known\": 0, \"UnKnown\": 0},\n",
    "                                                        {\"Known\": 0, \"UnKnown\": 0},\n",
    "                                                        {\"Known\": 0, \"UnKnown\": 0},\n",
    "                                                        {\"Known\": 0, \"UnKnown\": 0}]\n",
    "\n",
    "            state_tracker_field.append(state_in_initial_step)\n",
    "            batch_state_tracker_field.append(state_tracker_field)\n",
    "\n",
    "        return batch_state_tracker_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rozleFGSBZQ6"
   },
   "source": [
    "# Language Generator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "C6eyyupNBU71"
   },
   "outputs": [],
   "source": [
    "# coding = utf-8\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "#from src.config import DATA_ROOT, FRAUD, NON_FRAUD, NegativeSampledAnswerNum, Options, ShowUnknown, UnKnownUtterance\n",
    "\n",
    "with open(os.path.join(DATA_ROOT, \"idx2r.json\"), \"r\", encoding= 'utf-8') as f:\n",
    "    idx2r = json.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_ROOT, \"answersLibrary.json\"), \"r\", encoding= 'utf-8') as f:\n",
    "    answers_library = json.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_ROOT, \"languageTemplates.json\"), \"r\", encoding= 'utf-8') as f:\n",
    "    templates = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LanguageGenerator(object):\n",
    "    def __init__(self, language_generation_filed):\n",
    "        self.idx2r = idx2r\n",
    "        self.language_generation_filed = language_generation_filed\n",
    "        self.query_entity = None\n",
    "        self.answers_library = answers_library\n",
    "        self.templates = templates\n",
    "\n",
    "    def generate_question(self, system_action):\n",
    "        if \"worker\" in system_action.keys() and system_action[\"worker\"][\"sample_content\"] not in [FRAUD, NON_FRAUD]:\n",
    "            return self.query_entity[\"node_id\"], system_action[\"worker\"][\"sample_content\"]\n",
    "        elif \"manager\" in system_action.keys() and system_action[\"manager\"][\"sample_content\"] not in [FRAUD, NON_FRAUD]:\n",
    "            query_entity_node_id = system_action[\"manager\"][\"sample_content\"]\n",
    "            node = self.language_generation_filed[\"idx2node\"][str(query_entity_node_id)]\n",
    "            self.query_entity = {\"node\": node, \"node_id\": query_entity_node_id}\n",
    "            return None, None\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def _generate_sys_nl(self, h, r, t):\n",
    "        \"\"\"\n",
    "        Assume the h r t have been transferred to NL\n",
    "        return: natural language question, candidates, the correct answer option\n",
    "        \"\"\"\n",
    "        question = self.templates[str(r)]\n",
    "        question = random.choice(question)\n",
    "        question = re.sub(r\"\\$\\S\\$\", h, question)\n",
    "        \n",
    "\n",
    "        # To avoid user using exclusive method,\n",
    "        # the sampled negative answers should have similar appearance to the correct answer.\n",
    "        all_candidates = copy.deepcopy(self.answers_library[r])\n",
    "        print(all_candidates , t)\n",
    "        all_candidates.remove(t)\n",
    "        for c in all_candidates:\n",
    "            if t.find(c) != -1 or c.find(t) != -1:\n",
    "                all_candidates.remove(c)\n",
    "\n",
    "        answers_candidates = random.sample(all_candidates, NegativeSampledAnswerNum)\n",
    "        answers_candidates.append(t)\n",
    "        random.shuffle(answers_candidates)\n",
    "        answers_candidates.append(\"Not Sure\")\n",
    "        #candidates = \"  \".join([\" \".join([option, answer]) for option, answer in zip(Options, answers_candidates)])\n",
    "        candidates = [(option, answer) for option, answer in zip(Options, answers_candidates)]\n",
    "\n",
    "        return question, candidates\n",
    "\n",
    "    def generate_sys_nl(self, h, t):\n",
    "        r = self.language_generation_filed[\"h_t_to_r\"][str(h) + \" \" + str(t)]\n",
    "        h = self.language_generation_filed[\"idx2node\"][str(h)]\n",
    "        t = self.language_generation_filed[\"idx2node\"][str(t)]\n",
    "        r = self.idx2r[str(r)]\n",
    "\n",
    "        question, candidates = self._generate_sys_nl(h, r, t)\n",
    "        #return \"    \".join([question, candidates])\n",
    "        return question, candidates\n",
    "    def return_user_answer(choice):\n",
    "      #ans = input()\n",
    "\n",
    "      pass\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_user_nl(user_answer):\n",
    "        if user_answer != ShowUnknown:\n",
    "            return user_answer\n",
    "        else:\n",
    "            return UnKnownUtterance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7GbXfoPBys9"
   },
   "source": [
    "# Rollout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "SGlS5qPMBrQR"
   },
   "outputs": [],
   "source": [
    "def return_questions(graph_embed_field,\n",
    "                             policy_field,\n",
    "                             state_tracker_field,\n",
    "                             state_trackers,\n",
    "                             #users,\n",
    "                             language_generators,\n",
    "                             #dialogue_recorders,\n",
    "                             sample_flag,\n",
    "                             **models):\n",
    "  \n",
    "  \n",
    "  \n",
    "  # declare system mode\n",
    "  if sample_flag == \"random\":\n",
    "      mode = \"RuleWarmUp\"\n",
    "  else:\n",
    "      mode = \"RL\"\n",
    "\n",
    "  # rollout\n",
    "  if np.asarray([field[-1][\"episode_not_end\"] for field in state_tracker_field], dtype=np.bool).any():\n",
    "      # forward for GNN\n",
    "      graph_embed_feed_dict = build_graph_embed_inputs(graph_embed_field, state_tracker_field)\n",
    "      final_node_embed = models[\"gnn\"](graph_embed_feed_dict[\"initial_node_embed\"],\n",
    "                                        graph_embed_feed_dict[\"edges\"],\n",
    "                                        graph_embed_feed_dict[\"node_edges\"],\n",
    "                                        graph_embed_feed_dict[\"node_edge_mask\"])\n",
    "\n",
    "      # forward for workers state tracker\n",
    "      workers_state_tracker_feed_dict = build_workers_state_tracker_inputs(state_tracker_field, policy_field,\n",
    "                                                                            final_node_embed)\n",
    "      workers_state = models[\"workers_state_tracker\"](workers_state_tracker_feed_dict[\"known_one_hot\"],\n",
    "                                                      workers_state_tracker_feed_dict[\"unknown_one_hot\"],\n",
    "                                                      workers_state_tracker_feed_dict[\"known_differ_one_hot\"],\n",
    "                                                      workers_state_tracker_feed_dict[\"workers_qa_turn_one_hot\"],\n",
    "                                                      workers_state_tracker_feed_dict[\"workers_max_qa_turn_one_hot\"],\n",
    "                                                      workers_state_tracker_feed_dict[\"personal_nodes\"],\n",
    "                                                      workers_state_tracker_feed_dict[\"final_node_embed\"])\n",
    "\n",
    "      # forward for workers\n",
    "      workers_feed_dict = build_workers_inputs(workers_state, policy_field, final_node_embed)\n",
    "      _, workers_logits = models[\"workers\"](workers_feed_dict[\"workers_state\"],\n",
    "                                            workers_feed_dict[\"answer_nodes\"],\n",
    "                                            workers_feed_dict[\"graph_node_embedding\"])\n",
    "\n",
    "      # get the action mask\n",
    "      action_masks_dict = build_hierarchy_action_masks(state_tracker_field, mode)\n",
    "\n",
    "      # get the RL workers decision (batch_size, personal_node_num, 2)\n",
    "      rl_workers_probs = mask_softmax(workers_logits, action_masks_dict[\"rl_workers_action_mask\"], dim=2).detach()\n",
    "      rl_workers_decision = rl_workers_probs[:, :, -2:]\n",
    "\n",
    "      # forward for manager state tracker\n",
    "      manager_state_tracker_feed_dict = build_manager_state_tracker_inputs(graph_embed_field, final_node_embed,\n",
    "                                                                            rl_workers_decision,\n",
    "                                                                            state_tracker_field)\n",
    "      manager_state = models[\"manager_state_tracker\"](manager_state_tracker_feed_dict[\"feasible_personal_info_nodes\"],\n",
    "                                                      manager_state_tracker_feed_dict[\"workers_decision\"],\n",
    "                                                      manager_state_tracker_feed_dict[\"known_one_hot\"],\n",
    "                                                      manager_state_tracker_feed_dict[\"unknown_one_hot\"],\n",
    "                                                      manager_state_tracker_feed_dict[\"known_differ_one_hot\"],\n",
    "                                                      manager_state_tracker_feed_dict[\"total_qa_turn_one_hot\"],\n",
    "                                                      manager_state_tracker_feed_dict[\"personal_nodes\"],\n",
    "                                                      manager_state_tracker_feed_dict[\"final_node_embed\"])\n",
    "\n",
    "      # forward for manager\n",
    "      manager_feed_dict = build_hierarchy_manager_inputs(manager_state, workers_state, policy_field)\n",
    "      _, manager_logits = models[\"manager\"](manager_feed_dict[\"manager_state\"],\n",
    "                                            manager_feed_dict[\"workers_state\"])\n",
    "\n",
    "      # get current manager and workers policy distribution\n",
    "      if mode == \"RL\":\n",
    "          manager_probs = mask_softmax(manager_logits, action_masks_dict[\"rl_manager_action_mask\"], dim=1)\n",
    "          workers_probs = mask_softmax(workers_logits, action_masks_dict[\"rl_workers_action_mask\"], dim=2)\n",
    "      else:\n",
    "          manager_probs = mask_softmax(manager_logits, action_masks_dict[\"warm_up_manager_action_mask\"], dim=1)\n",
    "          workers_probs = mask_softmax(workers_logits, action_masks_dict[\"warm_up_workers_action_mask\"], dim=2)\n",
    "\n",
    "      # sample from probs\n",
    "      manager_action_probs = manager_probs.cpu().detach().numpy()\n",
    "      workers_action_probs = workers_probs.cpu().detach().numpy()\n",
    "      manager_actions = manager_feed_dict[\"personal_nodes\"].cpu().detach().numpy()\n",
    "      workers_actions = workers_feed_dict[\"answer_nodes\"].cpu().detach().numpy()\n",
    "      manager_sample_idxs, manager_sample_results, workers_sample_idxs, workers_sample_results = sample_hierarchy_rl(\n",
    "          manager_action_probs,\n",
    "          workers_action_probs,\n",
    "          manager_actions,\n",
    "          workers_actions,\n",
    "          sample_flag)\n",
    "      \n",
    "      for state_tracker, language_generator, manager_sample_idx, \\\n",
    "            manager_sample_result, manager_action_prob, workers_sample_idx, \\\n",
    "            workers_sample_result, workers_action_prob in zip(state_trackers,\n",
    "                                                              language_generators,\n",
    "                                                              manager_sample_idxs,\n",
    "                                                              manager_sample_results,\n",
    "                                                              manager_action_probs,\n",
    "                                                              workers_sample_idxs,\n",
    "                                                              workers_sample_results,\n",
    "                                                              workers_action_probs):\n",
    "              \n",
    "            questions, choices , state_in_new_step , system_action , language_generator , a_node , q_node  = state_tracker.move_a_step(\n",
    "                                          language_generator,\n",
    "                                          manager_sample_idx,\n",
    "                                          manager_sample_result,\n",
    "                                          manager_action_prob,\n",
    "                                          workers_sample_idx,\n",
    "                                          workers_sample_result,\n",
    "                                          workers_action_prob,  mode = \"RL\")\n",
    "            return questions, choices , state_in_new_step , system_action  , state_tracker , language_generator ,a_node ,q_node\n",
    "            \n",
    "  else:\n",
    "    for state_tracker in state_trackers:\n",
    "      del state_tracker.state_tracker_field[-1]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# def after_answer(questions, choices , state_in_new_step , system_action , state_tracker_field):\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nXHxylMBlWX"
   },
   "source": [
    "# Try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "pEsuMCxDBdy4"
   },
   "outputs": [],
   "source": [
    "# graph_preprocess = GraphPreprocessHRL(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "-EvQfY0hBnq3"
   },
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    graph_preprocess = GraphPreprocessHRL(batch_size=1)\n",
    "    for batch in graph_preprocess.generator('test', shuffle=False):\n",
    "      graph_embed_field = batch[\"graph_embed_field\"]\n",
    "      policy_field = batch[\"policy_field\"]\n",
    "      state_tracker_field = batch[\"state_tracker_field\"]\n",
    "      state_trackers = [StateTrackerHRL(field) for field in state_tracker_field]\n",
    "      language_generators = [LanguageGenerator(field) for field in batch[\"language_generation_filed\"]]\n",
    "      #print(batch['language_generation_filed'][1]['h_t_to_r']['0 9'])\n",
    "    return graph_embed_field , policy_field , state_tracker_field , state_trackers , language_generators\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJHC0g8IAwvp"
   },
   "source": [
    "#Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dh-mkH8WxO2p",
    "outputId": "4bb9873f-611d-4fef-cb21-cc96dcd9bdd2"
   },
   "outputs": [],
   "source": [
    "# # flask_ngrok_example.py\n",
    "# # !pwd\n",
    "# from flask import Flask, render_template, request\n",
    "# # from flask_ngrok import run_with_ngrok\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# # run_with_ngrok(app)  # Start ngrok when app is run\n",
    "\n",
    "# user_answer= ''\n",
    "# state_in_new_step = { }\n",
    "# system_action =  { }\n",
    "# state_tracker = 0\n",
    "# language_generator = 0\n",
    "# ####################################################################################\n",
    "# # @app.route('/', methods=[\"GET\", \"POST\"])\n",
    "# # def App():\n",
    "\n",
    "# #   return render_template( 'chatapp.html' )\n",
    "\n",
    "\n",
    "# @app.route('/', methods=[\"GET\"])\n",
    "# def show_questions():\n",
    " \n",
    "#    questions, choices , state_in_new_step2 , system_action2  , state_tracker2 , language_generator2   = return_questions(graph_embed_field,\n",
    "#                              policy_field,\n",
    "#                              state_tracker_field,\n",
    "#                              state_trackers,\n",
    "#                              #users,\n",
    "#                              language_generators,\n",
    "#                              #dialogue_recorders,\n",
    "#                              \"max\",\n",
    "#                              **models)\n",
    "#    print(language_generator2)\n",
    "  \n",
    "#    global state_in_new_step\n",
    "#    state_in_new_step= state_in_new_step2\n",
    "#    global system_action \n",
    "#    system_action= system_action2\n",
    "#    global state_tracker \n",
    "#    state_tracker = state_tracker2\n",
    "#    global language_generator\n",
    "#    language_generator  = language_generator2\n",
    "   \n",
    "\n",
    "#    return render_template('chatapp.html', \n",
    "#                           question= questions,\n",
    "#                           answer= choices\n",
    "                         \n",
    "#                          )\n",
    "  \n",
    "# @app.route('/answer' , methods= [\"POST\"])\n",
    "# def get_answer():\n",
    "#   if request.method =='POST':\n",
    "#     if 'submit_button' in request.form:\n",
    "#       user_answer=request.form['options']\n",
    "      \n",
    "#       global state_in_new_step\n",
    "#       state_in_new_step3 =  state_in_new_step \n",
    "#       global system_action \n",
    "#       system_action3 =  system_action\n",
    "#       global state_tracker\n",
    "#       state_tracker3 = state_tracker\n",
    "#       global language_generator\n",
    "#       language_generator3 = language_generator\n",
    "#       print(list(language_generator.language_generation_filed['idx2node'].keys()))\n",
    "      \n",
    "#       state_tracker3.continue_a_step(user_answer,state_in_new_step3,system_action3,language_generator3,choices)\n",
    "\n",
    "#       questions, choices , state_in_new_step4 , system_action4  , state_tracker4 , language_generator4  = return_questions(graph_embed_field,\n",
    "#                              policy_field,\n",
    "#                              state_tracker_field,\n",
    "#                              state_trackers,\n",
    "#                              #users,\n",
    "#                              language_generators,\n",
    "#                              #dialogue_recorders,\n",
    "#                              \"max\",\n",
    "#                              **models)\n",
    "      \n",
    "#       state_in_new_step = state_in_new_step4\n",
    "#       system_action = system_action4\n",
    "#       state_tracker = state_tracker4\n",
    "#       language_generator = language_generator4\n",
    "      \n",
    "#       return (questions, choices)\n",
    "  \n",
    "# # @app.route('/' , methods= [\"POST\"])\n",
    "# # def get_answer():\n",
    "# #   if request.method =='POST':\n",
    "# #     if 'submit_button' in request.form:\n",
    "# #       user_answer=request.form['options']\n",
    "      \n",
    "# #       global state_in_new_step\n",
    "# #       state_in_new_step3 =  state_in_new_step \n",
    "# #       global system_action\n",
    "# #       system_action3 =  system_action \n",
    "# #       global state_tracker_field\n",
    "# #       state_tracker_field3 =  state_tracker_field\n",
    "\n",
    "# #       state_tracker_field4 = after_answer(user_answer,  state_in_new_step3 , system_action3 , state_tracker_field3)\n",
    "\n",
    "# #       questions, choices , state_in_new_step4 , system_action4 , state_tracker_field4  = return_questions(graph_embed_field,\n",
    "# #                              policy_field,\n",
    "# #                              state_tracker_field4,\n",
    "# #                              state_trackers,\n",
    "# #                              #users,\n",
    "# #                              language_generators,\n",
    "# #                              #dialogue_recorders,\n",
    "# #                              sample_flag,\n",
    "# #                              **models)\n",
    "      \n",
    "# #       state_in_new_step = state_in_new_step4\n",
    "# #       system_action = system_action4\n",
    "# #       state_tracker_field = state_tracker_field4\n",
    "      \n",
    "# #       return (questions, choices)\n",
    "\n",
    "# ######################################################################################################################\n",
    "\n",
    "#   # new_rollout(graph_embed_field,\n",
    "#   #             policy_field,\n",
    "#   #             state_tracker_field,\n",
    "#   #             state_trackers,\n",
    "#   #             language_generators,\n",
    "#   #             'max',\n",
    "#   #             **models\n",
    "#   #             )\n",
    "#   # user_answer= ''\n",
    "\n",
    "#   # if request.method =='POST':\n",
    "#   #     if 'submit_button' in request.form:\n",
    "#   #         user_answer=request.form['options']\n",
    "#   #         print(user_answer) \n",
    "#   # #ans = request.form['options']\n",
    "#   # #print(user_answer)\n",
    "#   # return render_template('chatapp.html', \n",
    "#   #                        question= question,\n",
    "#   #                        answer= answer,\n",
    "#   #                        ans = user_answer)\n",
    "\n",
    "# if __name__ == '__main__' :\n",
    "#   app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [22/Jan/2022 14:39:16] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision step\n",
      "['اسواق عباد الرحمن', 'كارفور ماركت', 'مكتب تموين جمعيتي', 'أسواق الشيتانى'] كارفور ماركت\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Jan/2022 14:39:17] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [22/Jan/2022 14:39:20] \"\u001b[37mPOST /answer HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C USERANSWER\n",
      "wrong answer!!!  22\n",
      "['مركز براعم للاطفال', 'مستشفى صندلا المركزى.. Central Hospital of Sundela', 'عيادة د عبدالقادر حجازي ش ابراهيم المغازي ناصية ش الصداقة', 'Elaf Egypt'] Elaf Egypt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Jan/2022 14:39:31] \"\u001b[37mPOST /answer HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A USERANSWER\n",
      "wrong answer!!!  17\n",
      "['سوبر ماركت الربيع', 'سليم ماركت', 'منصة مكوك للمتاجر الالكترونية'] منصة مكوك للمتاجر الالكترونية\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Jan/2022 14:39:36] \"\u001b[37mPOST /answer HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B USERANSWER\n",
      "wrong answer!!!  21\n",
      "Decision step\n",
      "Decision step\n",
      "FRAUD !!!!!\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "state_in_new_step = 0\n",
    "system_action = 0\n",
    "state_tracker = 0\n",
    "language_generator = 0\n",
    "choices = 0\n",
    "a_node = 0\n",
    "q_node = 0\n",
    "graph_embed_field , policy_field , state_tracker_field , state_trackers , language_generators = 0,0,0,0,0\n",
    "@app.route('/', methods=[\"GET\"])\n",
    "def get_question():\n",
    "    global graph_embed_field , policy_field , state_tracker_field , state_trackers , language_generators\n",
    "    graph_embed_field , policy_field , state_tracker_field , state_trackers , language_generators = initialize()\n",
    "    \n",
    "    global state_in_new_step\n",
    "    global system_action\n",
    "    global state_tracker\n",
    "    global language_generator\n",
    "    global choices\n",
    "    global a_node \n",
    "    global q_node\n",
    "    questions = None \n",
    "    while questions is None:\n",
    "        questions, choices , state_in_new_step , system_action  , state_tracker , language_generator ,a_node ,q_node = return_questions(\n",
    "                                 graph_embed_field,\n",
    "                                 policy_field,\n",
    "                                 state_tracker_field,\n",
    "                                 state_trackers,\n",
    "                                 #users,\n",
    "                                 language_generators,\n",
    "                                 #dialogue_recorders,\n",
    "                                 \"max\",\n",
    "                                 **models)\n",
    "        \n",
    "        if questions is None:\n",
    "            case  = state_tracker.continue_a_step(None,state_in_new_step,system_action,language_generator,choices ,a_node,q_node)     \n",
    "            if case == 'continue':\n",
    "                pass\n",
    "            elif case == 'fraud':\n",
    "                return render_template('Fraud.html')\n",
    "            elif case == 'nonfraud':\n",
    "                return render_template('Non-Fraud.html')\n",
    "\n",
    "   \n",
    "        \n",
    "    return render_template('chatapp.html', question= questions , answer = choices)\n",
    "\n",
    "@app.route('/answer', methods=[\"POST\"])\n",
    "def get_answer():\n",
    "    global state_in_new_step\n",
    "    global system_action\n",
    "    global state_tracker\n",
    "    global language_generator\n",
    "    global choices\n",
    "    global a_node \n",
    "    global q_node\n",
    "    user_answer= ''\n",
    "    if request.method =='POST':\n",
    "        if 'submit_button' in request.form:\n",
    "            user_answer=request.form['options']\n",
    "            print(user_answer , 'USERANSWER') \n",
    "    case = state_tracker.continue_a_step(user_answer,state_in_new_step,system_action,language_generator, choices ,a_node,q_node)\n",
    "    if case == 'continue':\n",
    "        pass\n",
    "    elif case == 'fraud':\n",
    "            return render_template('Fraud.html')\n",
    "    elif case == 'nonfraud':\n",
    "            return render_template('Non-Fraud.html')\n",
    "\n",
    "            \n",
    "            \n",
    "    questions = None \n",
    "    while questions is None:\n",
    "        if (case == 'fraud') or (case == 'nonfraud'):\n",
    "            break\n",
    "        questions, choices , state_in_new_step , system_action  , state_tracker , language_generator ,a_node ,q_node = return_questions(\n",
    "                                 graph_embed_field,\n",
    "                                 policy_field,\n",
    "                                 state_tracker_field,\n",
    "                                 state_trackers,\n",
    "                                 #users,\n",
    "                                 language_generators,\n",
    "                                 #dialogue_recorders,\n",
    "                                 \"max\",\n",
    "                                 **models)\n",
    "        if questions is None:\n",
    "            case = state_tracker.continue_a_step(None,state_in_new_step,system_action,language_generator,choices ,a_node,q_node)\n",
    "            if case == 'continue':\n",
    "                pass\n",
    "            elif case == 'fraud':\n",
    "                    return render_template('Fraud.html')\n",
    "            elif case == 'nonfraud':\n",
    "                    \n",
    "                    return render_template('Non-Fraud.html')\n",
    "\n",
    "    \n",
    "    return render_template('chatapp.html', question= questions,answer= choices)\n",
    "\n",
    "# state_tracker.continue_a_step(user_answer,state_in_new_step,system_action,language_generator,choices ,a_node,q_node)\n",
    "if __name__ == '__main__' :\n",
    "  app.run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "noUser.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
